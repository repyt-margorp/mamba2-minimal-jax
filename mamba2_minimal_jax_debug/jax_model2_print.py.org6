"""
mamba2-minimal
==============

A minimal, single-file implementation of the Mamba-2 model in PyTorch.

> **Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality**
> Authors: Tri Dao, Albert Gu
> Paper: https://arxiv.org/abs/2405.21060
"""

import json
from dataclasses import dataclass
from typing import Iterable, NamedTuple, TypeAlias, cast

import torch
import torch.nn.functional as F
from einops import rearrange, repeat
from torch import LongTensor, Tensor, nn

Device: TypeAlias = str | torch.device | None

import jax
import jax.numpy as jnp
import numpy as np

def torch_to_jax(x):
    return jnp.array(x.detach().cpu().numpy())
def jax_to_torch(x):
    return torch.tensor(np.array(x), dtype=torch.float32)

def split_tensor(x, split_sizes, dim, use_jax=True):
    """
    Splits the tensor x into multiple tensors based on split_sizes along dimension dim.
    Uses JAX for splitting if use_jax=True, otherwise uses PyTorch.

    Args:
        x (Tensor or jnp.ndarray): The input tensor to split.
        split_sizes (list of int): The sizes to split the tensor into.
        dim (int): The dimension along which to split.
        use_jax (bool): Whether to use JAX for splitting.

    Returns:
        list of Tensor or jnp.ndarray: The list of split tensors.
    """
    if use_jax:
        splits = []
        start = 0
        for size in split_sizes:
            end = start + size
            # JAXでインデックスを生成
            indices = jnp.arange(start, end)
            # JAXのtakeを使用して分割
            split = jnp.take(x, indices=indices, axis=dim)
            splits.append(split)
            start = end
        return splits
    else:
        splits_torch = torch.split(
                jax_to_torch(x), split_sizes, dim=dim
        )
        splits_jax = []
        for spl in splits_torch:
            splits_jax.append(torch_to_jax(spl))
        return tuple(splits_jax)


@dataclass
class Mamba2Config:
    d_model: int  # model dimension (D)
    n_layer: int = 24  # number of Mamba-2 layers in the language model
    d_state: int = 128  # state dimension (N)
    d_conv: int = 4  # convolution kernel size
    expand: int = 2  # expansion factor (E)
    headdim: int = 64  # head dimension (P)
    chunk_size: int = 64  # matrix partition size (Q)
    vocab_size: int = 50277
    pad_vocab_size_multiple: int = 16

    def __post_init__(self):
        self.d_inner = self.expand * self.d_model
        assert self.d_inner % self.headdim == 0, "d_inner must be divisible by headdim"
        self.nheads = self.d_inner // self.headdim
        if self.vocab_size % self.pad_vocab_size_multiple != 0:
            self.vocab_size += (
                self.pad_vocab_size_multiple
                - self.vocab_size % self.pad_vocab_size_multiple
            )
        print(f"[Config] Initialized Mamba2Config with d_model={self.d_model}, "
              f"n_layer={self.n_layer}, d_state={self.d_state}, d_conv={self.d_conv}, "
              f"expand={self.expand}, headdim={self.headdim}, chunk_size={self.chunk_size}, "
              f"vocab_size={self.vocab_size}, pad_vocab_size_multiple={self.pad_vocab_size_multiple}")
        print(f"[Config] Computed d_inner={self.d_inner}, nheads={self.nheads}")


@dataclass
class InferenceCache:
    conv_state: jnp.ndarray  # (batch, d_inner + 2 * d_state, d_conv)
    ssm_state: jnp.ndarray   # (batch, nheads, headdim, d_state)

    @staticmethod
    #def alloc(batch_size: int, args: 'Mamba2Config') -> 'InferenceCache':
    def alloc(batch_size: int, args: Mamba2Config, device: Device = None) -> 'InferenceCache':
        print(f"[InferenceCache] Allocating InferenceCache for batch size: {batch_size}")

        conv_state = jnp.zeros(
            (batch_size, args.d_inner + 2 * args.d_state, args.d_conv)
        )
        print(f"[InferenceCache] conv_state shape: {conv_state.shape}")
        print(f"[InferenceCache] conv_state sample values: {conv_state.flatten()[:5]}")

        ssm_state = jnp.zeros(
            (batch_size, args.nheads, args.headdim, args.d_state)
        )
        print(f"[InferenceCache] ssm_state shape: {ssm_state.shape}")
        print(f"[InferenceCache] ssm_state sample values: {ssm_state.flatten()[:5]}")

        return InferenceCache(conv_state, ssm_state)


class Mamba2LMHeadModel(nn.Module):
    def __init__(self, args: Mamba2Config, device: Device = None, use_jax = True):
        super().__init__()
        self.args = args
        self.device = device
        self.use_jax = use_jax

        self.backbone = nn.ModuleDict(
            dict(
                embedding=nn.Embedding(args.vocab_size, args.d_model, device=device),
                layers=nn.ModuleList(
                    [
                        nn.ModuleDict(
                            dict(
                                mixer=Mamba2(args, device=device),
                                norm=RMSNorm(args.d_model, device=device),
                            )
                        )
                        for _ in range(args.n_layer)
                    ]
                ),
                norm_f=RMSNorm(args.d_model, device=device),
            )
        )
        self.lm_head = nn.Linear(
            args.d_model, args.vocab_size, bias=False, device=device
        )
        self.lm_head.weight = self.backbone.embedding.weight

    @staticmethod
    def from_pretrained(huggingface_model_id: str, device: Device = None):
        from transformers.utils import CONFIG_NAME, WEIGHTS_NAME
        from transformers.utils.hub import cached_file

        print(f"[Mamba2LMHeadModel] Loading pre-trained model from HuggingFace ID: {huggingface_model_id}")
        config_path = cached_file(huggingface_model_id, CONFIG_NAME)
        assert config_path, "Failed to get HuggingFace config file"
        state_dict_path = cached_file(huggingface_model_id, WEIGHTS_NAME)
        assert state_dict_path, "Failed to get HuggingFace state dict file"

        config = json.load(open(config_path))
        args = Mamba2Config(
            d_model=config["d_model"],
            n_layer=config["n_layer"],
            vocab_size=config["vocab_size"],
            pad_vocab_size_multiple=config.get("pad_vocab_size_multiple", 16),
        )

        map_location = "cpu" if device is None else device
        state_dict = torch.load(
            state_dict_path, map_location=map_location, weights_only=True
        )
        print("[Mamba2LMHeadModel] Loaded state dict.")
        model = Mamba2LMHeadModel(args, device=device)
        model.load_state_dict(state_dict)
        model.eval()
        print("[Mamba2LMHeadModel] Model loaded and set to evaluation mode.")

        # Print shapes of some key parameters for debugging
        print("[Mamba2LMHeadModel] Loaded model parameters:")
        for name, param in model.named_parameters():
            print(f"  {name}: {param.shape}")
            # Print sample values
            if param.numel() > 0:
                print(f"    Sample values from {name}: {param.flatten()[:5]}")
            break  # Remove this break to see more parameters

        return model

    def forward(
        self, input_ids: LongTensor, h: list[InferenceCache] | list[None] | None = None
    ) -> tuple[LongTensor, list[InferenceCache]]:
        """
        Arguments
            input_ids: (batch, seqlen) tokens from EleutherAI/gpt-neox-20b tokenizer
            h: hidden states for inference step. If present the constant-time
               (wrt sequence length) inference path will be taken, input_ids
               should have shape (batch, 1) containing the next batch of prompt
               token.

        Return (logits, h)
            logits: (batch, seqlen, vocab_size)
            h: updated inference cache after processing input_ids
        """
        print(f"[Mamba2LMHeadModel] Forward pass input_ids shape: {input_ids.shape}")
        print(f"[Mamba2LMHeadModel] input_ids sample values: {input_ids.flatten()[:5]}")
        seqlen = input_ids.shape[1]

        if h is None:
            h = [None for _ in range(self.args.n_layer)]
            print("[Mamba2LMHeadModel] Initialized hidden states.")

        x = self.backbone.embedding(input_ids)
        x = torch_to_jax(x)
        print(f"[Mamba2LMHeadModel] Embedding output shape: {x.shape}")
        print(f"[Mamba2LMHeadModel] Embedding sample values: {x.flatten()[:5]}")

        for i, layer in enumerate(self.backbone.layers):
            print(f"[Mamba2LMHeadModel] Processing layer {i + 1}/{self.args.n_layer}")
            #x_jax = torch_to_jax(x)
            y, h[i] = layer.mixer(
                layer.norm(x),
                #jax_to_torch(layer.norm(x_jax)),
                h[i])
            print(f"  [Layer {i + 1}] Output shape after mixer: {y.shape}")
            print(f"  [Layer {i + 1}] Output sample values after mixer: {y.flatten()[:5]}")
            x = y + x
            print(f"  [Layer {i + 1}] Residual connection shape: {x.shape}")
            print(f"  [Layer {i + 1}] Residual connection sample values: {x.flatten()[:5]}")

        #x_jax = torch_to_jax(x)
        x_jax = x
        x_jax = self.backbone.norm_f(x_jax)
        x = jax_to_torch(x_jax)
        print(f"[Mamba2LMHeadModel] Final backbone norm output shape: {x.shape}")
        print(f"[Mamba2LMHeadModel] Final backbone norm output sample values: {x.flatten()[:5]}")
        if self.use_jax:
            print("[Mamba2LMHeadModel] Using JAX for lm_head")
            lm_head_weight_jax = torch_to_jax(self.lm_head.weight)
            x_jax = torch_to_jax(x)
            logits_jax = jnp.dot(x_jax, lm_head_weight_jax.T)  # lm_headの重みを使ったドット積
            logits = jax_to_torch(logits_jax)
        else:
            print("[Mamba2LMHeadModel] Using PyTorch for lm_head")
            logits = self.lm_head(x)
        print(f"[Mamba2LMHeadModel] Logits shape: {logits.shape}")
        print(f"[Mamba2LMHeadModel] Logits sample values: {logits.flatten()[:5]}")

        return logits[:, :seqlen], cast(list[InferenceCache], h)

    def generate(
        self,
        input_ids: LongTensor,
        max_new_length: int = 20,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 1.0,
        eos_token_id: int = 0,
    ) -> Iterable[tuple[int, list[InferenceCache]]]:
        prefix, tokens = input_ids[:-1], input_ids[-1:].unsqueeze(0)

        # Process prompt
        # The input sequence to forward (non-inference path) must have length multiple that of chunk_size.
        # We split out excess tokens so that n_chunked tokens can be processed by one forward call and
        # process the rest in multiple inference steps.
        n_chunked = (prefix.shape[0] // self.args.chunk_size) * self.args.chunk_size
        if n_chunked > 0:
            _, h = self(prefix[:n_chunked].unsqueeze(0), None)
        else:
            h = [
                #InferenceCache.alloc(1, self.args, device=self.device)
                InferenceCache.alloc(1, self.args)
                for _ in range(self.args.n_layer)
            ]
        for i in range(n_chunked, prefix.shape[0]):
            _, h = self(prefix[i : i + 1].unsqueeze(0), h)

        # Generate
        for _ in range(max_new_length):
            with torch.no_grad():
                out, h = self(tokens, h)
            logits = out[0, -1]

            # Tempreture scaling の適用
            if temperature != 1.0:
                logits = logits / temperature

            if self.use_jax:
                logits_jax = torch_to_jax(logits)
                probs_jax = jax.nn.softmax(logits_jax)
                probs = jax_to_torch(probs_jax)
            else:
                probs = F.softmax(logits, dim=-1)

            # 確率を基にtop_k, top_pを適用する
            if top_k > 0:
                indices_to_remove = probs < torch.topk(probs, k=top_k)[0][-1]
                probs[indices_to_remove] = 0  # 削除対象の確率を0に設定
            if top_p < 1.0:
                sorted_probs, sorted_indices = torch.sort(probs, descending=True)

                if self.use_jax:
                    sorted_probs_jax = torch_to_jax(sorted_probs)
                    cum_probs_jax = jnp.cumsum(sorted_probs_jax)
                    cum_probs = jax_to_torch(cum_probs_jax)
                else:
                    cum_probs = torch.cumsum(sorted_probs, dim=-1)

                sorted_indices_to_remove = cum_probs > top_p
                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                sorted_indices_to_remove[0] = False
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                probs[indices_to_remove] = 0  # 削除対象の確率を0に設定

            # ここで次のトークンを確率に基づいてサンプリング
            next_token = torch.multinomial(probs, num_samples=1)

            if next_token.item() == eos_token_id:
                return

            tokens = next_token.unsqueeze(0)
            yield cast(int, next_token.item()), h


class Mamba2(nn.Module):
    def __init__(self, args: Mamba2Config, device: Device = None, use_jax=True):
    #def __init__(self, args: Mamba2Config, device: Device = None, use_jax=False):
        super().__init__()
        self.args = args
        self.device = device
        self.use_jax = use_jax

        print("[Mamba2] Initializing Mamba2 mixer...")
        d_in_proj = 2 * args.d_inner + 2 * args.d_state + args.nheads
        print(f"  [Mamba2] d_in_proj: {d_in_proj}")
        self.in_proj = nn.Linear(args.d_model, d_in_proj, bias=False, device=device)
        print(f"  [Mamba2] Initialized in_proj with input_dim={args.d_model}, output_dim={d_in_proj}")
        print(f"  [Mamba2] in_proj.weight shape: {self.in_proj.weight.shape}")
        print(f"  [Mamba2] in_proj.weight sample values: {self.in_proj.weight.flatten()[:5]}")

        conv_dim = args.d_inner + 2 * args.d_state
        self.conv_dim = args.d_inner + 2 * args.d_state
        self.conv1d = nn.Conv1d(
            in_channels=conv_dim,
            out_channels=conv_dim,
            kernel_size=args.d_conv,
            groups=conv_dim,
            padding=args.d_conv - 1,
            device=device,
        )
        print(f"  [Mamba2] Initialized conv1d with in_channels={conv_dim}, "
              f"out_channels={conv_dim}, kernel_size={args.d_conv}")
        print(f"  [Mamba2] conv1d.weight shape: {self.conv1d.weight.shape}")
        print(f"  [Mamba2] conv1d.weight sample values: {self.conv1d.weight.flatten()[:5]}")

        self.dt_bias = nn.Parameter(torch.empty(args.nheads, device=device))
        print(f"  [Mamba2] Initialized dt_bias with shape: {self.dt_bias.shape}")
        print(f"  [Mamba2] dt_bias sample values: {self.dt_bias.flatten()[:5]}")
        self.A_log = nn.Parameter(torch.empty(args.nheads, device=device))
        print(f"  [Mamba2] Initialized A_log with shape: {self.A_log.shape}")
        print(f"  [Mamba2] A_log sample values: {self.A_log.flatten()[:5]}")
        self.D = nn.Parameter(torch.empty(args.nheads, device=device))
        print(f"  [Mamba2] Initialized D with shape: {self.D.shape}")
        print(f"  [Mamba2] D sample values: {self.D.flatten()[:5]}")
        print("  [Mamba2] Initialized A_log and D parameters.")

        self.norm = RMSNorm(args.d_inner, device=device)
        print("  [Mamba2] Initialized RMSNorm.")

        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=False, device=device)
        print("  [Mamba2] Initialized out_proj.")
        print(f"  [Mamba2] out_proj.weight shape: {self.out_proj.weight.shape}")
        print(f"  [Mamba2] out_proj.weight sample values: {self.out_proj.weight.flatten()[:5]}")

        # Initialize parameters
        self._init_parameters()

    def _init_parameters(self):
        print("[Mamba2] Initializing parameters with normal distribution.")
        nn.init.normal_(self.dt_bias, mean=0.0, std=0.02)
        print(f"  [Mamba2] dt_bias after init: {self.dt_bias.flatten()[:5]}")
        nn.init.normal_(self.A_log, mean=0.0, std=0.02)
        print(f"  [Mamba2] A_log after init: {self.A_log.flatten()[:5]}")
        nn.init.normal_(self.D, mean=0.0, std=0.02)
        print(f"  [Mamba2] D after init: {self.D.flatten()[:5]}")
        nn.init.normal_(self.in_proj.weight, mean=0.0, std=0.02)
        print(f"  [Mamba2] in_proj.weight after init: {self.in_proj.weight.flatten()[:5]}")
        nn.init.normal_(self.conv1d.weight, mean=0.0, std=0.02)
        print(f"  [Mamba2] conv1d.weight after init: {self.conv1d.weight.flatten()[:5]}")
        nn.init.normal_(self.out_proj.weight, mean=0.0, std=0.02)
        print(f"  [Mamba2] out_proj.weight after init: {self.out_proj.weight.flatten()[:5]}")
        print("[Mamba2] Parameters initialized.")

    def forward(self, u: Tensor, h: InferenceCache | None = None):
        """
        Arguments
            u: (batch, seqlen, d_model) input. seqlen should be a multiple of chunk_size.
            h: hidden states for inference step. Initialized to 0s if not present.

        Return (y, h)
            y: (batch, seqlen, d_model) output
            h: updated inference cache after processing `u`
        """
        if h:
            return self.step(u, h)

        A_log = torch_to_jax(self.A_log)
        D = torch_to_jax(self.D)
        dt_bias_jax = torch_to_jax(self.dt_bias)

        A = -jnp.exp(A_log)  # JAX computation # (nheads,)
        print(f"  [Mamba2] A shape: {A.shape}")
        print(f"  [Mamba2] A sample values: {A.flatten()[:5]}")

        # in_proj
        zxbcdt = jax.numpy.dot(u, torch_to_jax(self.in_proj.weight).T)
        print(f"  [Mamba2] in_proj output shape: {zxbcdt.shape}")
        print(f"  [Mamba2] in_proj output sample values: {zxbcdt.flatten()[:5]}")

        split_sizes = [self.args.d_inner, self.args.d_inner + 2 * self.args.d_state, self.args.nheads]
        z, xBC, dt = split_tensor(zxbcdt, split_sizes, dim=-1, use_jax=self.use_jax)
        print(f"  [Mamba2] Split shapes (step) -> z: {z.shape}, xBC: {xBC.shape}, dt: {dt.shape}")
        print(f"  [Mamba2] z (step) sample values: {z.flatten()[:5]}")
        print(f"  [Mamba2] xBC (step) sample values: {xBC.flatten()[:5]}")
        print(f"  [Mamba2] dt (step) sample values: {dt.flatten()[:5]}")

        dt = jax.nn.softplus(dt + dt_bias_jax)  # JAXでsoftplus
        print(f"  [Mamba2] dt after softplus shape: {dt.shape}")
        print(f"  [Mamba2] dt after softplus sample values: {dt.flatten()[:5]}")

        if self.use_jax:
            # Calculate padding amount
            pad_amount = self.args.d_conv - u.shape[1]
            if pad_amount > 0:
                # Positive padding: pad the tensor
                conv_state = rearrange(xBC, 'b l d -> b d l')  # (batch, conv_dim, seqlen)
                conv_state_padded = jnp.pad(conv_state, ((0, 0), (0, 0), (pad_amount, 0)), mode='constant')
                conv_state = conv_state_padded[..., -self.args.d_conv:]  # Keep last d_conv elements
            elif pad_amount < 0:
                # Negative padding: slice the tensor
                conv_state = rearrange(xBC, 'b l d -> b d l')  # (batch, conv_dim, seqlen)
                conv_state = conv_state[..., -self.args.d_conv:]  # Keep last d_conv elements
            else:
                # Zero padding: use the tensor as is
                conv_state = rearrange(xBC, 'b l d -> b d l')[..., -self.args.d_conv:]
            print(f"  [Mamba2] conv_state shape after padding/slicing: {conv_state.shape}")
            print(f"  [Mamba2] conv_state sample values: {conv_state.flatten()[:5]}")
            conv_state = jax_to_torch(conv_state)
        else:
            # PyTorch implementation (as in the original code)
            xBC_torch = jax_to_torch(xBC)
            conv_state = F.pad(
                rearrange(xBC_torch, "b l d -> b d l"), (self.args.d_conv - u.shape[1], 0)
            )
            print(f"  [Mamba2] conv_state shape after padding: {conv_state.shape}")
            print(f"  [Mamba2] conv_state after padding sample values: {conv_state.flatten()[:5]}")

        if self.use_jax:
            # Prepare convolution weights
            conv_weight_jax = torch_to_jax(self.conv1d.weight)  # (conv_dim, 1, kernel_size)
            conv_weight_jax = conv_weight_jax.squeeze(1).T[:, None, :]  # (kernel_size, 1, conv_dim)
            print(f"  [Mamba2] conv_weight_jax shape: {conv_weight_jax.shape}")
            print(f"  [Mamba2] conv_weight_jax sample values: {conv_weight_jax.flatten()[:5]}")
    
            # Perform depthwise convolution using xBC directly
            def depthwise_conv(inputs, filters):
                outputs = jax.lax.conv_general_dilated(
                    inputs,
                    filters,
                    window_strides=(1,),
                    padding=[(self.args.d_conv - 1, 0)],  # Left padding
                    dimension_numbers=('NWC', 'WIO', 'NWC'),
                    feature_group_count=self.conv_dim,
                )
                return outputs

            xBC_conv = depthwise_conv(xBC, conv_weight_jax)  # (batch, seqlen, conv_dim)
            if self.conv1d.bias is not None:
                xBC_conv += torch_to_jax(self.conv1d.bias)
                print(f"  [Mamba2] Added conv1d bias shape: {xBC_conv.shape}")
                print(f"  [Mamba2] xBC_conv after adding bias sample values: {xBC_conv.flatten()[:5]}")
    
            xBC = silu(xBC_conv)
            print(f"  [Mamba2] xBC after convolution shape: {xBC.shape}")
            print(f"  [Mamba2] xBC after convolution sample values: {xBC.flatten()[:5]}")
        else:
            xBC_torch = jax_to_torch(xBC)
            xBC = silu(
                torch_to_jax(self.conv1d(xBC_torch.transpose(1, 2)).transpose(1, 2)[:, : u.shape[1], :])
            )  # (batch, seqlen, d_inner + 2 * d_state))
            print(f"  [Mamba2] conv1d output shape: {xBC.shape}")
            print(f"  [Mamba2] conv1d output sample values: {xBC.flatten()[:5]}")

        split_sizes = [self.args.d_inner, self.args.d_state, self.args.d_state]
        x, B, C = split_tensor(xBC, split_sizes, dim=-1, use_jax=self.use_jax)
        print(f"  [Mamba2] Split conv1d output -> x: {x.shape}, B: {B.shape}, C: {C.shape}")
        print(f"  [Mamba2] x sample values: {x.flatten()[:5]}")
        print(f"  [Mamba2] B sample values: {B.flatten()[:5]}")
        print(f"  [Mamba2] C sample values: {C.flatten()[:5]}")

        x = rearrange(x, "b l (h p) -> b l h p", p=self.args.headdim)
        print(f"  [Mamba2] x after rearrange shape: {x.shape}")
        print(f"  [Mamba2] x after rearrange sample values: {x.flatten()[:5]}")

        y, ssm_state = ssd(
            x * jnp.expand_dims(dt, axis=-1),
            A * dt,
            rearrange(B, "b l n -> b l 1 n"),
            rearrange(C, "b l n -> b l 1 n"),
            self.args.chunk_size,
            device=self.device,
        )
        ssm_state = jax_to_torch(ssm_state)
        print(f"  [Mamba2] ssd output y shape: {y.shape}, ssm_state shape: {ssm_state.shape}")
        print(f"  [Mamba2] y sample values: {y.flatten()[:5]}")
        print(f"  [Mamba2] ssm_state sample values: {ssm_state.flatten()[:5]}")

        y = y + jnp.expand_dims(D, axis=-1) * x
        print(f"  [Mamba2] y after adding D scaling shape: {y.shape}")
        print(f"  [Mamba2] y after adding D scaling sample values: {y.flatten()[:5]}")

        y = rearrange(y, "b l h p -> b l (h p)")
        print(f"  [Mamba2] y after rearrange to (b, l, d_inner): {y.shape}")
        print(f"  [Mamba2] y after rearrange sample values: {y.flatten()[:5]}")

        y = self.norm(y, z)
        print(f"  [Mamba2] y after RMSNorm shape: {y.shape}")
        print(f"  [Mamba2] y after RMSNorm sample values: {y.flatten()[:5]}")

        # out_proj
        y = jax.numpy.dot(y, torch_to_jax(self.out_proj.weight).T)
        print(f"  [Mamba2] y after out_proj shape: {y.shape}")
        print(f"  [Mamba2] y after out_proj sample values: {y.flatten()[:5]}")

        h = InferenceCache(torch_to_jax(conv_state), torch_to_jax(ssm_state))
        return y, h

    def step(self, u: Tensor, h: InferenceCache) -> tuple[Tensor, InferenceCache]:
        """Take a single inference step for the current input and hidden state

        Unlike attention-based models, RNN-based models (eg Mamba) do not need
        to look back at all the past tokens to generate a new token. Instead a
        hidden state (initialized to 0s initially) is updated for each input and
        passed to the next inference step. This means that the total inference
        time is linear with respect to the sequence length instead of quadratic
        in attention's case.

        Arguments
            u: (batch, 1, d_model)
            h: initial/running hidden state

        Return (y, h)
            y: (batch, 1, d_model)
            h: updated hidden state
        """
        print("[Mamba2] Performing full forward pass.")
        print("[Mamba2] Using JAX for Mamba2 computation")
        u_jax = u  # PyTorchのテンソルをJAXのテンソルに変換
        A_log_jax = torch_to_jax(self.A_log)
        D_jax = torch_to_jax(self.D)
        dt_bias_jax = torch_to_jax(self.dt_bias)

        #assert u.shape[1] == 1, "Only one token can be decoded per inference step"
        assert u_jax.shape[1] == 1, "Only one token can be decoded per inference step"
        print("[Mamba2] Performing single inference step.")

        # in_proj計算
        zxbcdt = jnp.dot(u_jax.squeeze(1), torch_to_jax(self.in_proj.weight).T)  # JAXで線形変換

        print(f"  [Mamba2] in_proj (step) output shape: {zxbcdt.shape}")
        print(f"  [Mamba2] in_proj (step) output sample values: {zxbcdt.flatten()[:5]}")

        # テンソルの分割をsplit_tensorを使用して切り替える
        split_sizes = [self.args.d_inner, self.args.d_inner + 2 * self.args.d_state, self.args.nheads]
        z, xBC, dt = split_tensor(zxbcdt, split_sizes, dim=-1, use_jax=self.use_jax)
        print(f"  [Mamba2] Split shapes (step) -> z: {z.shape}, xBC: {xBC.shape}, dt: {dt.shape}")
        print(f"  [Mamba2] z (step) sample values: {z.flatten()[:5]}")
        print(f"  [Mamba2] xBC (step) sample values: {xBC.flatten()[:5]}")
        print(f"  [Mamba2] dt (step) sample values: {dt.flatten()[:5]}")

        # Advance convolution input
        rolled_conv_state = jnp.roll(h.conv_state, shift=-1, axis=-1)
        updated_conv_state = rolled_conv_state.at[:, :, -1].set(jax_to_torch(xBC))
        h = InferenceCache(conv_state=updated_conv_state, ssm_state=h.ssm_state)
        print(f"  [Mamba2] conv_state updated shape: {h.conv_state.shape}")
        print(f"  [Mamba2] conv_state updated sample values: {h.conv_state.flatten()[:5]}")

        ################
        # Convolution step
        ################
        print("[Mamba2] Using JAX for convolution and rearrange")
        conv_weight_jax = torch_to_jax(self.conv1d.weight)
        h_conv_state_jax = h.conv_state
        conv_weight_jax = rearrange(conv_weight_jax, "d 1 w -> d w")
        xBC = jnp.sum(h_conv_state_jax * conv_weight_jax, axis=-1)
        print(f"  [Mamba2] xBC after convolution sum shape: {xBC.shape}")
        print(f"  [Mamba2] xBC after convolution sum sample values: {xBC.flatten()[:5]}")

        if self.conv1d.bias is not None:
            xBC += torch_to_jax(self.conv1d.bias)
            print(f"  [Mamba2] Added conv1d bias shape: {xBC.shape}")
            print(f"  [Mamba2] xBC after adding bias sample values: {xBC.flatten()[:5]}")

        xBC = silu(xBC)
        print(f"  [Mamba2] xBC after silu shape: {xBC.shape}")
        print(f"  [Mamba2] xBC after silu sample values: {xBC.flatten()[:5]}")

        split_sizes = [self.args.d_inner, self.args.d_state, self.args.d_state]
        x, B, C = split_tensor(xBC, split_sizes, dim=-1, use_jax=self.use_jax)
        print(f"  [Mamba2] Split xBC -> x: {x.shape}, B: {B.shape}, C: {C.shape}")
        print(f"  [Mamba2] x (step) sample values: {x.flatten()[:5]}")
        print(f"  [Mamba2] B (step) sample values: {B.flatten()[:5]}")
        print(f"  [Mamba2] C (step) sample values: {C.flatten()[:5]}")

        # Aの計算
        A = -jnp.exp(A_log_jax)  # JAXでのA計算
        print(f"  [Mamba2] A shape: {A.shape}")
        print(f"  [Mamba2] A sample values: {A.flatten()[:5]}")

        ###############
        # SSM step
        ###############
        dt = jax.nn.softplus(dt + dt_bias_jax)  # JAXでsoftplus
        print(f"  [Mamba2] dt after softplus (step) shape: {dt.shape}")
        print(f"  [Mamba2] dt after softplus (step) sample values: {dt.flatten()[:5]}")

        # dAの計算
        dA = jnp.exp(dt * A)  # JAXでの計算
        print(f"  [Mamba2] dA shape: {dA.shape}")
        print(f"  [Mamba2] dA sample values: {dA.flatten()[:5]}")

        # rearrange 実行前の x の形状を確認
        print(f"[Mamba2] Before rearrange, x shape: {x.shape}")
        print("[Mamba2] Using JAX for rearrange")
        x = rearrange(x, "b (h p) -> b h p", p=self.args.headdim)
        print(f"  [Mamba2] x after rearrange (step) shape: {x.shape}")
        print(f"  [Mamba2] x after rearrange (step) sample values: {x.flatten()[:5]}")

        dBx = jnp.einsum("bh, bn, bhp -> bhpn", dt, B, x)  # JAXでの計算
        print(f"  [Mamba2] dBx shape: {dBx.shape}")
        print(f"  [Mamba2] dBx sample values: {dBx.flatten()[:5]}")
        print(f"  [Mamba2] dBx shape: {dBx.shape}")
        print(f"  [Mamba2] dBx sample values: {dBx.flatten()[:5]}")

        # JAXでのssm_state更新
        ssm_state_jax = h.ssm_state
        updated_ssm_state_jax = ssm_state_jax *  rearrange(dA, "b h -> b h 1 1") + dBx
        # ここでh.ssm_stateに直接代入するのではなく、新しいhを生成
        h = InferenceCache(conv_state=h.conv_state, ssm_state=updated_ssm_state_jax)  # 新しいhオブジェクトを生成
        print(f"  [Mamba2] ssm_state updated shape: {h.ssm_state.shape}")
        print(f"  [Mamba2] ssm_state updated sample values: {h.ssm_state.flatten()[:5]}")

        # JAXでのeinsum
        y = jnp.einsum("bhpn, bn -> bhp", updated_ssm_state_jax, C)
        print(f"  [Mamba2] y after einsum shape: {y.shape}")
        print(f"  [Mamba2] y after einsum sample values: {y.flatten()[:5]}")

        # JAXでのrearrange
        D = torch_to_jax(self.D)
        y = y + rearrange(D, "h -> h 1") * x
        print(f"  [Mamba2] y after adding D scaling (step) shape: {y.shape}")
        print(f"  [Mamba2] y after adding D scaling (step) sample values: {y.flatten()[:5]}")

        y = rearrange(y, "b h p -> b (h p)")
        print(f"  [Mamba2] y after rearrange to (b, d_inner) shape: {y.shape}")
        print(f"  [Mamba2] y after rearrange to (b, d_inner) sample values: {y.flatten()[:5]}")

        # RMSNormの処理は既存のコードをそのまま使用
        y = self.norm(y, z)
        print(f"  [Mamba2] y after RMSNorm (step) shape: {y.shape}")
        print(f"  [Mamba2] y after RMSNorm (step) sample values: {y.flatten()[:5]}")

        y = jnp.dot(y, torch_to_jax(self.out_proj.weight).T) 
        print(f"  [Mamba2] y after out_proj (step) shape: {y.shape}")
        print(f"  [Mamba2] y after out_proj (step) sample values: {y.flatten()[:5]}")

        y = jnp.expand_dims(y, axis=1)

        return y, h


def segsum(x: jnp.ndarray, device: Device = None) -> jnp.ndarray:
    """Stable segment sum calculation.

    `exp(segsum(A))` produces a 1-semiseparable matrix, which is equivalent to a scalar SSM.

    Source: https://github.com/state-spaces/mamba/blob/219f03c840d5a44e7d42e4e728134834fddccf45/mamba_ssm/modules/ssd_minimal.py#L23-L32
    """
    T = x.shape[-1]  # 時間次元のサイズ

    # JAXでrepeat相当の処理
    x = repeat(x, "... d -> ... d e", e=T)
    #x = jnp.tile(x[..., None], (1, 1, T))
    # 下三角行列のマスクをJAXで作成
    mask_jax = jnp.tril(jnp.ones((T, T), dtype=bool), k=-1)
    # JAXでmasked_fill相当の処理
    x = jnp.where(mask_jax, x, 0)
    # JAXで累積和 (cumsum)
    x_segsum = jnp.cumsum(x, axis=-2)
    # 再度マスクを適用 (下三角部分のみ残す)
    mask_jax = jnp.tril(jnp.ones((T, T), dtype=bool), k=0)
    x_segsum = jnp.where(mask_jax, x_segsum, -jnp.inf)

    return x_segsum


from typing import Optional, Tuple
def ssd(
    x: jnp.ndarray,
    A: jnp.ndarray,
    B: jnp.ndarray,
    C: jnp.ndarray,
    chunk_size: int,
    initial_states: Optional[jnp.ndarray] = None,
    device: Optional[torch.device] = None
) -> Tuple[jnp.ndarray, jnp.ndarray]:
    """
    Structured State Space Duality (SSD) using JAX for internal computations.

    Arguments:
        x: (batch, seqlen, n_heads, d_head) - Input tensor
        A: (batch, seqlen, n_heads) - A tensor
        B: (batch, seqlen, n_heads, d_state) - B tensor
        C: (batch, seqlen, n_heads, d_state) - C tensor
        chunk_size: int - Size of each chunk
        initial_states: Optional[(batch, 1, n_heads, d_state, ...)] - Initial states
        device: Optional[torch.device] - Device to place the output tensors

    Returns:
        Y: (batch, seqlen, n_heads, d_head) - Output tensor
        final_state: (batch, n_heads, d_state, ...) - Final state tensor
    """
    print("[ssd_jax] Starting SSD computation with JAX...")
    assert x.shape[1] % chunk_size == 0, "Sequence length must be divisible by chunk_size."

    # Rearrange into chunks using JAX's equivalent of einops.rearrange
    x, A, B, C = [
        rearrange(m, "b (c l) ... -> b c l ...", l=chunk_size) for m in (x, A, B, C)
    ]
    print(f"  [ssd_jax] After chunking shapes -> x: {x.shape}, A: {A.shape}, B: {B.shape}, C: {C.shape}")
    print(f"  [ssd_jax] x after chunking sample values: {x.flatten()[:5]}")
    print(f"  [ssd_jax] A after chunking sample values: {A.flatten()[:5]}")
    print(f"  [ssd_jax] B after chunking sample values: {B.flatten()[:5]}")
    print(f"  [ssd_jax] C after chunking sample values: {C.flatten()[:5]}")

    # Rearrange A for cumulative sum
    A = rearrange(A, "b c l h -> b h c l")
    print(f"  [ssd_jax] A rearranged shape: {A.shape}")
    print(f"  [ssd_jax] A rearranged sample values: {A.flatten()[:5]}")

    A_cumsum = jnp.cumsum(A, axis=-1)
    print(f"  [ssd_jax] A_cumsum shape: {A_cumsum.shape}")
    print(f"  [ssd_jax] A_cumsum sample values: {A_cumsum.flatten()[:5]}")

    # 1. Compute the output for each intra-chunk (diagonal blocks)
    L = jnp.exp(segsum(x=A, device=device))
    print(f"  [ssd_jax] L shape: {L.shape}")
    print(f"  [ssd_jax] L sample values: {L.flatten()[:5]}")

    # Perform Einstein summation using JAX
    Y_diag = jnp.einsum("bclhn, bcshn, bhcls, bcshp -> bclhp", C, B, L, x)
    print(f"  [ssd_jax] Y_diag shape: {Y_diag.shape}")
    print(f"  [ssd_jax] Y_diag sample values: {Y_diag.flatten()[:5]}")

    # 2. Compute the state for each intra-chunk
    decay_states = jnp.exp(A_cumsum[..., -1:] - A_cumsum)
    print(f"  [ssd_jax] decay_states shape: {decay_states.shape}")
    print(f"  [ssd_jax] decay_states sample values: {decay_states.flatten()[:5]}")

    states = jnp.einsum("bclhn, bhcl, bclhp -> bchpn", B, decay_states, x)
    print(f"  [ssd_jax] states shape: {states.shape}")
    print(f"  [ssd_jax] states sample values: {states.flatten()[:5]}")

    # 3. Compute the inter-chunk SSM recurrence
    if initial_states is None:
        initial_states = jnp.zeros_like(states[:, :1])
        print("  [ssd_jax] Initialized initial_states with zeros.")
        print(f"  [ssd_jax] initial_states sample values: {initial_states.flatten()[:5]}")

    states = jnp.concatenate([initial_states, states], axis=1)
    print(f"  [ssd_jax] states after concatenation shape: {states.shape}")
    print(f"  [ssd_jax] states after concatenation sample values: {states.flatten()[:5]}")

    # Compute decay_chunk
    A_cumsum_padded = jnp.pad(A_cumsum[:, :, :, -1], ((0,0), (0,0), (1,0)), mode='constant')
    decay_chunk = jnp.exp(segsum(x=A_cumsum_padded, device=device))
    print(f"  [ssd_jax] decay_chunk shape: {decay_chunk.shape}")
    print(f"  [ssd_jax] decay_chunk sample values: {decay_chunk.flatten()[:5]}")

    new_states = jnp.einsum("bhzc, bchpn -> bzhpn", decay_chunk, states)
    print(f"  [ssd_jax] new_states shape: {new_states.shape}")
    print(f"  [ssd_jax] new_states sample values: {new_states.flatten()[:5]}")

    states, final_state = new_states[:, :-1], new_states[:, -1]
    print(f"  [ssd_jax] states shape after splitting: {states.shape}, final_state shape: {final_state.shape}")
    print(f"  [ssd_jax] states after splitting sample values: {states.flatten()[:5]}")
    print(f"  [ssd_jax] final_state sample values: {final_state.flatten()[:5]}")

    # 4. Compute state -> output conversion per chunk
    state_decay_out = jnp.exp(A_cumsum)
    print(f"  [ssd_jax] state_decay_out shape: {state_decay_out.shape}")
    print(f"  [ssd_jax] state_decay_out sample values: {state_decay_out.flatten()[:5]}")

    Y_off = jnp.einsum("bclhn, bchpn, bhcl -> bclhp", C, states, state_decay_out)
    print(f"  [ssd_jax] Y_off shape: {Y_off.shape}")
    print(f"  [ssd_jax] Y_off sample values: {Y_off.flatten()[:5]}")

    # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)
    Y = Y_diag + Y_off
    Y = rearrange(Y, "b c l h p -> b (c l) h p")
    print(f"  [ssd_jax] Y after adding Y_diag and Y_off shape: {Y.shape}")
    print(f"  [ssd_jax] Y after adding Y_diag and Y_off sample values: {Y.flatten()[:5]}")

    return Y, final_state



class RMSNorm(torch.nn.Module):
    def __init__(self, d: int, eps: float = 1e-5, device=None, use_jax=True):
        """Gated Root Mean Square Layer Normalization

        Paper: https://arxiv.org/abs/1910.07467
        """
        super().__init__()
        self.eps = eps
        self.weight = torch.nn.Parameter(torch.ones(d, device=device))
        print(f"[RMSNorm] Initialized RMSNorm with d={d}, eps={eps}")
        print(f"[RMSNorm] weight shape: {self.weight.shape}")
        print(f"[RMSNorm] weight sample values: {self.weight.flatten()[:5]}")

    def forward(self, x, z=None):
        if z is not None:
            x = x * silu(z)
            print(f"[RMSNorm] x after gated scaling shape: {x.shape}")
            print(f"[RMSNorm] x after gated scaling sample values: {x.flatten()[:5]}")

        mean_sq = jnp.mean(jnp.square(x), axis=-1, keepdims=True)
        rsqrt = jnp.sqrt(1 / (mean_sq + self.eps))
        x_normalized = x * rsqrt * torch_to_jax(self.weight)

        print(f"[RMSNorm] x_normalized shape: {x_normalized.shape}")
        print(f"[RMSNorm] x_normalized sample values: {x_normalized.flatten()[:5]}")
        return x_normalized

def silu(x):
    """Applies the Sigmoid Linear Unit (SiLU), element-wise.

    This function supports both JAX and PyTorch computations depending on the use_jax flag.
    """
    return x * jax.nn.sigmoid(x)
