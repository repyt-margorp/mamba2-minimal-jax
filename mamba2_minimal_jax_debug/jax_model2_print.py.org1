"""
mamba2-minimal
==============

A minimal, single-file implementation of the Mamba-2 model in PyTorch with enhanced
debugging capabilities to print tensor shapes and sample values during execution.

> **Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality**
> Authors: Tri Dao, Albert Gu
> Paper: https://arxiv.org/abs/2405.21060
"""

import json
from dataclasses import dataclass
from typing import Iterable, NamedTuple, TypeAlias, cast

import torch
import torch.nn.functional as F
#from einops import rearrange, repeat
from torch import LongTensor, Tensor, nn

Device: TypeAlias = str | torch.device | None

import jax
import jax.numpy as jnp
import numpy as np

# PyTorchのテンソルをJAXのテンソルに変換する関数
def torch_to_jax(x):
    return jnp.array(x.detach().cpu().numpy())

# JAXのテンソルをPyTorchのテンソルに変換する関数
def jax_to_torch(x):
    return torch.tensor(np.array(x), dtype=torch.float32)

def split_tensor(x, split_sizes, dim, use_jax=False):
    """
    Splits the tensor x into multiple tensors based on split_sizes along dimension dim.
    Uses JAX for splitting if use_jax=True, otherwise uses PyTorch.

    Args:
        x (Tensor or jnp.ndarray): The input tensor to split.
        split_sizes (list of int): The sizes to split the tensor into.
        dim (int): The dimension along which to split.
        use_jax (bool): Whether to use JAX for splitting.

    Returns:
        list of Tensor or jnp.ndarray: The list of split tensors.
    """
    if use_jax:
        splits = []
        start = 0
        # PyTorchのテンソルをJAXの配列に変換
        x_jax = torch_to_jax(x)
        for size in split_sizes:
            end = start + size
            # JAXでインデックスを生成
            indices = jnp.arange(start, end)
            # JAXのtakeを使用して分割
            split_jax = jnp.take(x_jax, indices=indices, axis=dim)
            # 分割後のJAX配列をPyTorchのテンソルに再変換
            split_torch = jax_to_torch(split_jax)
            splits.append(split_torch)
            start = end
        return splits
    else:
        return torch.split(x, split_sizes, dim=dim)


@dataclass
class Mamba2Config:
    d_model: int  # model dimension (D)
    n_layer: int = 24  # number of Mamba-2 layers in the language model
    d_state: int = 128  # state dimension (N)
    d_conv: int = 4  # convolution kernel size
    expand: int = 2  # expansion factor (E)
    headdim: int = 64  # head dimension (P)
    chunk_size: int = 64  # matrix partition size (Q)
    vocab_size: int = 50277
    pad_vocab_size_multiple: int = 16

    def __post_init__(self):
        self.d_inner = self.expand * self.d_model
        assert self.d_inner % self.headdim == 0, "d_inner must be divisible by headdim"
        self.nheads = self.d_inner // self.headdim
        if self.vocab_size % self.pad_vocab_size_multiple != 0:
            self.vocab_size += (
                self.pad_vocab_size_multiple
                - self.vocab_size % self.pad_vocab_size_multiple
            )
        print(f"[Config] Initialized Mamba2Config with d_model={self.d_model}, "
              f"n_layer={self.n_layer}, d_state={self.d_state}, d_conv={self.d_conv}, "
              f"expand={self.expand}, headdim={self.headdim}, chunk_size={self.chunk_size}, "
              f"vocab_size={self.vocab_size}, pad_vocab_size_multiple={self.pad_vocab_size_multiple}")
        print(f"[Config] Computed d_inner={self.d_inner}, nheads={self.nheads}")


class InferenceCache(NamedTuple):
    conv_state: Tensor  # (batch, d_inner + 2 * d_state, d_conv)
    ssm_state: Tensor  # (batch, nheads, headdim, d_state)

    @staticmethod
    def alloc(batch_size: int, args: Mamba2Config, device: Device = None):
        print(f"[InferenceCache] Allocating InferenceCache for batch size: {batch_size}")
        conv_state = torch.zeros(
            batch_size, args.d_inner + 2 * args.d_state, args.d_conv, device=device
        )
        print(f"[InferenceCache] conv_state shape: {conv_state.shape}")
        print(f"[InferenceCache] conv_state sample values: {conv_state.flatten()[:5]}")
        ssm_state = torch.zeros(
            batch_size, args.nheads, args.headdim, args.d_state, device=device
        )
        print(f"[InferenceCache] ssm_state shape: {ssm_state.shape}")
        print(f"[InferenceCache] ssm_state sample values: {ssm_state.flatten()[:5]}")
        return InferenceCache(conv_state, ssm_state)


class Mamba2LMHeadModel(nn.Module):
    #def __init__(self, args: Mamba2Config, device: Device = None, use_jax=False):
    def __init__(self, args: Mamba2Config, device: Device = None, use_jax=True):
        super().__init__()
        self.args = args
        self.device = device
        self.use_jax = use_jax

        print("[Mamba2LMHeadModel] Initializing backbone...")
        self.backbone = nn.ModuleDict(
            dict(
                embedding=nn.Embedding(args.vocab_size, args.d_model, device=device),
                layers=nn.ModuleList(
                    [
                        nn.ModuleDict(
                            dict(
                                mixer=Mamba2(args, device=device),
                                norm=RMSNorm(args.d_model, device=device),
                            )
                        )
                        for _ in range(args.n_layer)
                    ]
                ),
                norm_f=RMSNorm(args.d_model, device=device),
            )
        )
        print(f"[Mamba2LMHeadModel] Initialized embedding with vocab_size={args.vocab_size} and d_model={args.d_model}")
        for i, layer in enumerate(self.backbone.layers):
            print(f"[Mamba2LMHeadModel] Initialized layer {i + 1}/{self.args.n_layer}")

        self.lm_head = nn.Linear(
            args.d_model, args.vocab_size, bias=False, device=device
        )
        self.lm_head.weight = self.backbone.embedding.weight
        print("[Mamba2LMHeadModel] Initialized LM Head and tied weights with embedding.")

    @staticmethod
    def from_pretrained(huggingface_model_id: str, device: Device = None):
        from transformers.utils import CONFIG_NAME, WEIGHTS_NAME
        from transformers.utils.hub import cached_file

        print(f"[Mamba2LMHeadModel] Loading pre-trained model from HuggingFace ID: {huggingface_model_id}")
        config_path = cached_file(huggingface_model_id, CONFIG_NAME)
        assert config_path, "Failed to get HuggingFace config file"
        state_dict_path = cached_file(huggingface_model_id, WEIGHTS_NAME)
        assert state_dict_path, "Failed to get HuggingFace state dict file"

        config = json.load(open(config_path))
        args = Mamba2Config(
            d_model=config["d_model"],
            n_layer=config["n_layer"],
            vocab_size=config["vocab_size"],
            pad_vocab_size_multiple=config.get("pad_vocab_size_multiple", 16),
        )

        map_location = "cpu" if device is None else device
        state_dict = torch.load(
            state_dict_path, map_location=map_location, weights_only=True
        )
        print("[Mamba2LMHeadModel] Loaded state dict.")
        model = Mamba2LMHeadModel(args, device=device)
        model.load_state_dict(state_dict)
        model.eval()
        print("[Mamba2LMHeadModel] Model loaded and set to evaluation mode.")

        # Print shapes of some key parameters for debugging
        print("[Mamba2LMHeadModel] Loaded model parameters:")
        for name, param in model.named_parameters():
            print(f"  {name}: {param.shape}")
            # Print sample values
            if param.numel() > 0:
                print(f"    Sample values from {name}: {param.flatten()[:5]}")
            break  # Remove this break to see more parameters

        return model

    def forward(
        self, input_ids: LongTensor, h: list[InferenceCache] | list[None] | None = None
    ) -> tuple[LongTensor, list[InferenceCache]]:
        """
        Arguments
            input_ids: (batch, seqlen) tokens from EleutherAI/gpt-neox-20b tokenizer
            h: hidden states for inference step. If present the constant-time
               (wrt sequence length) inference path will be taken, input_ids
               should have shape (batch, 1) containing the next batch of prompt
               token.

        Return (logits, h)
            logits: (batch, seqlen, vocab_size)
            h: updated inference cache after processing input_ids
        """
        print(f"[Mamba2LMHeadModel] Forward pass input_ids shape: {input_ids.shape}")
        print(f"[Mamba2LMHeadModel] input_ids sample values: {input_ids.flatten()[:5]}")
        seqlen = input_ids.shape[1]

        if h is None:
            h = [None for _ in range(self.args.n_layer)]
            print("[Mamba2LMHeadModel] Initialized hidden states.")

        x = self.backbone.embedding(input_ids)
        print(f"[Mamba2LMHeadModel] Embedding output shape: {x.shape}")
        print(f"[Mamba2LMHeadModel] Embedding sample values: {x.flatten()[:5]}")

        for i, layer in enumerate(self.backbone.layers):
            print(f"[Mamba2LMHeadModel] Processing layer {i + 1}/{self.args.n_layer}")
            y, h[i] = layer.mixer(layer.norm(x), h[i])
            print(f"  [Layer {i + 1}] Output shape after mixer: {y.shape}")
            print(f"  [Layer {i + 1}] Output sample values after mixer: {y.flatten()[:5]}")
            x = y + x
            print(f"  [Layer {i + 1}] Residual connection shape: {x.shape}")
            print(f"  [Layer {i + 1}] Residual connection sample values: {x.flatten()[:5]}")

        x = self.backbone.norm_f(x)
        print(f"[Mamba2LMHeadModel] Final backbone norm output shape: {x.shape}")
        print(f"[Mamba2LMHeadModel] Final backbone norm output sample values: {x.flatten()[:5]}")
        logits = self.lm_head(x)
        print(f"[Mamba2LMHeadModel] Logits shape: {logits.shape}")
        print(f"[Mamba2LMHeadModel] Logits sample values: {logits.flatten()[:5]}")

        return logits[:, :seqlen], cast(list[InferenceCache], h)

    def generate(
        self,
        input_ids: LongTensor,
        max_new_length: int = 20,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 1.0,
        eos_token_id: int = 0,
    ) -> Iterable[tuple[int, list[InferenceCache]]]:
        prefix, tokens = input_ids[:-1], input_ids[-1:].unsqueeze(0)

        # Process prompt
        # The input sequence to forward (non-inference path) must have length multiple that of chunk_size.
        # We split out excess tokens so that n_chunked tokens can be processed by one forward call and
        # process the rest in multiple inference steps.
        n_chunked = (prefix.shape[0] // self.args.chunk_size) * self.args.chunk_size
        if n_chunked > 0:
            _, h = self(prefix[:n_chunked].unsqueeze(0), None)
        else:
            h = [
                InferenceCache.alloc(1, self.args, device=self.device)
                for _ in range(self.args.n_layer)
            ]
        for i in range(n_chunked, prefix.shape[0]):
            _, h = self(prefix[i : i + 1].unsqueeze(0), h)

        # Generate
        for _ in range(max_new_length):
            with torch.no_grad():
                out, h = self(tokens, h)
            logits = out[0, -1]

            # Tempreture scaling の適用
            if temperature != 1.0:
                logits = logits / temperature

            if self.use_jax:
                logits_jax = torch_to_jax(logits)
                probs_jax = jax.nn.softmax(logits_jax)
                probs = jax_to_torch(probs_jax)
            else:
                probs = F.softmax(logits, dim=-1)

            # 確率を基にtop_k, top_pを適用する
            if top_k > 0:
                indices_to_remove = probs < torch.topk(probs, k=top_k)[0][-1]
                probs[indices_to_remove] = 0  # 削除対象の確率を0に設定
            if top_p < 1.0:
                sorted_probs, sorted_indices = torch.sort(probs, descending=True)

                if self.use_jax:
                    sorted_probs_jax = torch_to_jax(sorted_probs)
                    cum_probs_jax = jnp.cumsum(sorted_probs_jax)
                    cum_probs = jax_to_torch(cum_probs_jax)
                else:
                    cum_probs = torch.cumsum(sorted_probs, dim=-1)

                sorted_indices_to_remove = cum_probs > top_p
                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                sorted_indices_to_remove[0] = False
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                probs[indices_to_remove] = 0  # 削除対象の確率を0に設定

            # ここで次のトークンを確率に基づいてサンプリング
            next_token = torch.multinomial(probs, num_samples=1)

            if next_token.item() == eos_token_id:
                return

            tokens = next_token.unsqueeze(0)
            yield cast(int, next_token.item()), h


class Mamba2(nn.Module):
    #def __init__(self, args: Mamba2Config, device: Device = None, use_jax=False):
    def __init__(self, args: Mamba2Config, device: Device = None, use_jax=True):
        super().__init__()
        self.args = args
        self.device = device
        self.use_jax = use_jax

        print("[Mamba2] Initializing Mamba2 mixer...")
        d_in_proj = 2 * args.d_inner + 2 * args.d_state + args.nheads
        print(f"  [Mamba2] d_in_proj: {d_in_proj}")
        self.in_proj = nn.Linear(args.d_model, d_in_proj, bias=False, device=device)
        print(f"  [Mamba2] Initialized in_proj with input_dim={args.d_model}, output_dim={d_in_proj}")
        print(f"  [Mamba2] in_proj.weight shape: {self.in_proj.weight.shape}")
        print(f"  [Mamba2] in_proj.weight sample values: {self.in_proj.weight.flatten()[:5]}")

        conv_dim = args.d_inner + 2 * args.d_state
        self.conv1d = nn.Conv1d(
            in_channels=conv_dim,
            out_channels=conv_dim,
            kernel_size=args.d_conv,
            groups=conv_dim,
            padding=args.d_conv - 1,
            device=device,
        )
        print(f"  [Mamba2] Initialized conv1d with in_channels={conv_dim}, "
              f"out_channels={conv_dim}, kernel_size={args.d_conv}")
        print(f"  [Mamba2] conv1d.weight shape: {self.conv1d.weight.shape}")
        print(f"  [Mamba2] conv1d.weight sample values: {self.conv1d.weight.flatten()[:5]}")

        self.dt_bias = nn.Parameter(torch.empty(args.nheads, device=device))
        print(f"  [Mamba2] Initialized dt_bias with shape: {self.dt_bias.shape}")
        print(f"  [Mamba2] dt_bias sample values: {self.dt_bias.flatten()[:5]}")
        self.A_log = nn.Parameter(torch.empty(args.nheads, device=device))
        print(f"  [Mamba2] Initialized A_log with shape: {self.A_log.shape}")
        print(f"  [Mamba2] A_log sample values: {self.A_log.flatten()[:5]}")
        self.D = nn.Parameter(torch.empty(args.nheads, device=device))
        print(f"  [Mamba2] Initialized D with shape: {self.D.shape}")
        print(f"  [Mamba2] D sample values: {self.D.flatten()[:5]}")
        print("  [Mamba2] Initialized A_log and D parameters.")

        self.norm = RMSNorm(args.d_inner, device=device, use_jax=self.use_jax)
        print("  [Mamba2] Initialized RMSNorm.")

        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=False, device=device)
        print("  [Mamba2] Initialized out_proj.")
        print(f"  [Mamba2] out_proj.weight shape: {self.out_proj.weight.shape}")
        print(f"  [Mamba2] out_proj.weight sample values: {self.out_proj.weight.flatten()[:5]}")

        # Initialize parameters
        self._init_parameters()

    def _init_parameters(self):
        print("[Mamba2] Initializing parameters with normal distribution.")
        nn.init.normal_(self.dt_bias, mean=0.0, std=0.02)
        print(f"  [Mamba2] dt_bias after init: {self.dt_bias.flatten()[:5]}")
        nn.init.normal_(self.A_log, mean=0.0, std=0.02)
        print(f"  [Mamba2] A_log after init: {self.A_log.flatten()[:5]}")
        nn.init.normal_(self.D, mean=0.0, std=0.02)
        print(f"  [Mamba2] D after init: {self.D.flatten()[:5]}")
        nn.init.normal_(self.in_proj.weight, mean=0.0, std=0.02)
        print(f"  [Mamba2] in_proj.weight after init: {self.in_proj.weight.flatten()[:5]}")
        nn.init.normal_(self.conv1d.weight, mean=0.0, std=0.02)
        print(f"  [Mamba2] conv1d.weight after init: {self.conv1d.weight.flatten()[:5]}")
        nn.init.normal_(self.out_proj.weight, mean=0.0, std=0.02)
        print(f"  [Mamba2] out_proj.weight after init: {self.out_proj.weight.flatten()[:5]}")
        print("[Mamba2] Parameters initialized.")

    def forward(self, u: Tensor, h: InferenceCache | None = None):
        """
        Arguments
            u: (batch, seqlen, d_model) input. seqlen should be a multiple of chunk_size.
            h: hidden states for inference step. Initialized to 0s if not present.

        Return (y, h)
            y: (batch, seqlen, d_model)
            h: updated inference cache after processing u
        """
        if h:
            print("[Mamba2] Performing inference step.")
            return self.step(u, h)

        print("[Mamba2] Performing full forward pass.")
        if self.use_jax:
            print("[Mamba2] Using JAX for Mamba2 computation")
        else:
            print("[Mamba2] Using PyTorch for Mamba2 computation")

        # Handle JAX or PyTorch computation
        if self.use_jax:
            A_log = torch_to_jax(self.A_log)
            D = torch_to_jax(self.D)
            u = torch_to_jax(u)
            A = -jnp.exp(A_log)  # JAX computation
        else:
            A = -torch.exp(self.A_log)  # PyTorch computation

        print(f"  [Mamba2] A shape: {A.shape}")
        print(f"  [Mamba2] A sample values: {A.flatten()[:5]}")

        if self.use_jax:
            zxbcdt = jax.numpy.dot(u, torch_to_jax(self.in_proj.weight).T)
        else:
            zxbcdt = self.in_proj(u)  # (batch, seqlen, d_in_proj)

        print(f"  [Mamba2] in_proj output shape: {zxbcdt.shape}")
        print(f"  [Mamba2] in_proj output sample values: {zxbcdt.flatten()[:5]}")

        split_sizes = [self.args.d_inner, self.args.d_inner + 2 * self.args.d_state, self.args.nheads]
        z, xBC, dt = split_tensor(zxbcdt, split_sizes, dim=-1, use_jax=self.use_jax)
        print(f"  [Mamba2] Split shapes -> z: {z.shape}, xBC: {xBC.shape}, dt: {dt.shape}")
        print(f"  [Mamba2] z sample values: {z.flatten()[:5]}")
        print(f"  [Mamba2] xBC sample values: {xBC.flatten()[:5]}")
        print(f"  [Mamba2] dt sample values: {dt.flatten()[:5]}")

        if self.use_jax:
            dt_jax = jax.nn.softplus(torch_to_jax(dt) + dt_bias_jax)  # JAXでsoftplus
            dt = jax_to_torch(dt_jax)  # 結果をPyTorchに戻す
        else:
            dt = F.softplus(dt + self.dt_bias)  # PyTorchの場合
        print(f"  [Mamba2] dt after softplus shape: {dt.shape}")
        print(f"  [Mamba2] dt after softplus sample values: {dt.flatten()[:5]}")

        if self.use_jax:
            print("[Mamba2] Using JAX for rearrange and padding")
            # JAXでの次元変換: rearrange(xBC, "b l d -> b d l") に相当する操作
            xBC_jax = torch_to_jax(xBC)
            xBC_jax = jnp.transpose(xBC_jax, (0, 2, 1))  # "b l d -> b d l" に相当
            # JAXでのパディング
            conv_state_jax = jnp.pad(xBC_jax, ((0, 0), (0, 0), (self.args.d_conv - u.shape[1], 0)))
            # 結果をPyTorchのテンソルに戻す
            conv_state = jax_to_torch(conv_state_jax)
        else:
            print("[Mamba2] Using PyTorch for rearrange and padding")
            conv_state = F.pad(
                rearrange(xBC, "b l d -> b d l"), (self.args.d_conv - u.shape[1], 0)
            )
        print(f"  [Mamba2] conv_state shape after padding: {conv_state.shape}")
        print(f"  [Mamba2] conv_state after padding sample values: {conv_state.flatten()[:5]}")

        if self.use_jax:
            print("[Mamba2] Using JAX for conv1d and SiLU computation")
            xBC_jax = torch_to_jax(xBC.transpose(1, 2))  # JAXに変換
            conv_weight_jax = torch_to_jax(self.conv1d.weight)  # Conv1Dの重みをJAXに変換

            # JAXでの畳み込み操作 (conv1d相当)
            xBC_jax = jax.lax.conv_general_dilated(
                xBC_jax,
                conv_weight_jax,
                window_strides=(1,),
                padding='SAME',
                dimension_numbers=('NCW', 'OIW', 'NCW')
            )
            xBC_jax = jax_to_torch(xBC_jax.transpose(1, 2)[:, : u.shape[1], :])  # PyTorchに戻す

            # SiLUをJAXで実行
            xBC = silu(xBC_jax, use_jax=self.use_jax)

        else:
            print("[Mamba2] Using PyTorch for conv1d and SiLU computation")
            # PyTorchでのconv1dとSiLU
            xBC = silu(
                self.conv1d(xBC.transpose(1, 2)).transpose(1, 2)[:, : u.shape[1], :],
                use_jax=self.use_jax
            )
        print(f"  [Mamba2] conv1d output shape: {xBC.shape}")
        print(f"  [Mamba2] conv1d output sample values: {xBC.flatten()[:5]}")

        split_sizes = [self.args.d_inner, self.args.d_state, self.args.d_state]
        x, B, C = split_tensor(xBC, split_sizes, dim=-1, use_jax=self.use_jax)
        print(f"  [Mamba2] Split conv1d output -> x: {x.shape}, B: {B.shape}, C: {C.shape}")
        print(f"  [Mamba2] x sample values: {x.flatten()[:5]}")
        print(f"  [Mamba2] B sample values: {B.flatten()[:5]}")
        print(f"  [Mamba2] C sample values: {C.flatten()[:5]}")

        if self.use_jax:
            print("[Mamba2] Using JAX for rearrange, ssd, and norm computation")

            # JAXでのrearrange相当の操作
            x_jax = torch_to_jax(x)
            x_jax = jnp.reshape(x_jax, (x_jax.shape[0], x_jax.shape[1], self.args.nheads, self.args.headdim))
            print(f"  [Mamba2] x after rearrange shape (JAX): {x_jax.shape}")
            print(f"  [Mamba2] x after rearrange sample values (JAX): {x_jax.flatten()[:5]}")

            # JAXでのssd呼び出し
            dt_jax = torch_to_jax(dt)
            A_jax = torch_to_jax(A)
            B_jax = jnp.reshape(torch_to_jax(B), (B.shape[0], B.shape[1], 1, B.shape[2]))  # equivalent to rearrange
            C_jax = jnp.reshape(torch_to_jax(C), (C.shape[0], C.shape[1], 1, C.shape[2]))  # equivalent to rearrange

            y_jax, ssm_state_jax = ssd_jax(
                x_jax * jnp.expand_dims(dt_jax, axis=-1),
                A_jax * dt_jax,
                B_jax,
                C_jax,
                self.args.chunk_size,
                device=self.device,
            )
            y = jax_to_torch(y_jax)
            ssm_state = jax_to_torch(ssm_state_jax)
            print(f"  [Mamba2] ssd output y shape (JAX): {y.shape}, ssm_state shape (JAX): {ssm_state.shape}")
            print(f"  [Mamba2] y sample values (JAX): {y.flatten()[:5]}")
            print(f"  [Mamba2] ssm_state sample values (JAX): {ssm_state.flatten()[:5]}")

            # JAXでのD scaling
            D_jax = torch_to_jax(self.D)
            y_jax = y_jax + jnp.expand_dims(D_jax, axis=-1) * x_jax
            y = jax_to_torch(y_jax)
            print(f"  [Mamba2] y after adding D scaling shape (JAX): {y.shape}")
            print(f"  [Mamba2] y after adding D scaling sample values (JAX): {y.flatten()[:5]}")

            # JAXでのrearrange (b l h p -> b l (h p))
            y_jax = jnp.reshape(y_jax, (y_jax.shape[0], y_jax.shape[1], -1))
            y = jax_to_torch(y_jax)
            print(f"  [Mamba2] y after rearrange to (b, l, d_inner) shape (JAX): {y.shape}")
            print(f"  [Mamba2] y after rearrange sample values (JAX): {y.flatten()[:5]}")

            # RMSNormの処理
            y = self.norm(y, z)
            print(f"  [Mamba2] y after RMSNorm shape: {y.shape}")
            print(f"  [Mamba2] y after RMSNorm sample values: {y.flatten()[:5]}")

            # JAXでのout_proj
            y_jax = jnp.dot(torch_to_jax(y), torch_to_jax(self.out_proj.weight).T)
            y = jax_to_torch(y_jax)
            print(f"  [Mamba2] y after out_proj shape (JAX): {y.shape}")
            print(f"  [Mamba2] y after out_proj sample values (JAX): {y.flatten()[:5]}")
        else:
            # PyTorchの場合
            x = rearrange(x, "b l (h p) -> b l h p", p=self.args.headdim)
            print(f"  [Mamba2] x after rearrange shape: {x.shape}")
            print(f"  [Mamba2] x after rearrange sample values: {x.flatten()[:5]}")

            y, ssm_state = ssd(
                x * dt.unsqueeze(-1),
                A * dt,
                rearrange(B, "b l n -> b l 1 n"),
                rearrange(C, "b l n -> b l 1 n"),
                self.args.chunk_size,
                device=self.device,
            )
            print(f"  [Mamba2] ssd output y shape: {y.shape}, ssm_state shape: {ssm_state.shape}")
            print(f"  [Mamba2] y sample values: {y.flatten()[:5]}")
            print(f"  [Mamba2] ssm_state sample values: {ssm_state.flatten()[:5]}")

            y = y + x * self.D.unsqueeze(-1)
            print(f"  [Mamba2] y after adding D scaling shape: {y.shape}")
            print(f"  [Mamba2] y after adding D scaling sample values: {y.flatten()[:5]}")

            y = rearrange(y, "b l h p -> b l (h p)")
            print(f"  [Mamba2] y after rearrange to (b, l, d_inner): {y.shape}")
            print(f"  [Mamba2] y after rearrange sample values: {y.flatten()[:5]}")

            y = self.norm(y, z)
            print(f"  [Mamba2] y after RMSNorm shape: {y.shape}")
            print(f"  [Mamba2] y after RMSNorm sample values: {y.flatten()[:5]}")

            y = self.out_proj(y)
            print(f"  [Mamba2] y after out_proj shape: {y.shape}")
            print(f"  [Mamba2] y after out_proj sample values: {y.flatten()[:5]}")

        # InferenceCacheの更新 (JAX/PyTorch共通)
        h = InferenceCache(conv_state, ssm_state)
        print(f"  [Mamba2] Updated InferenceCache: conv_state shape {h.conv_state.shape}, ssm_state shape {h.ssm_state.shape}")
        print(f"  [Mamba2] conv_state sample values: {h.conv_state.flatten()[:5]}")
        print(f"  [Mamba2] ssm_state sample values: {h.ssm_state.flatten()[:5]}")

        return y, h

    def step(self, u: Tensor, h: InferenceCache) -> tuple[Tensor, InferenceCache]:
        """Take a single inference step for the current input and hidden state

        Unlike attention-based models, RNN-based models (eg Mamba) do not need
        to look back at all the past tokens to generate a new token. Instead a
        hidden state (initialized to 0s initially) is updated for each input and
        passed to the next inference step. This means that the total inference
        time is linear with respect to the sequence length instead of quadratic
        in attention's case.

        Arguments
            u: (batch, 1, d_model)
            h: initial/running hidden state

        Return (y, h)
            y: (batch, 1, d_model)
            h: updated hidden state
        """
        print("[Mamba2] Performing full forward pass.")
        if self.use_jax:
            print("[Mamba2] Using JAX for Mamba2 computation")
            u_jax = torch_to_jax(u)  # PyTorchのテンソルをJAXのテンソルに変換
            A_log_jax = torch_to_jax(self.A_log)
            D_jax = torch_to_jax(self.D)
            dt_bias_jax = torch_to_jax(self.dt_bias)
        else:
            print("[Mamba2] Using PyTorch for Mamba2 computation")

        assert u.shape[1] == 1, "Only one token can be decoded per inference step"
        print("[Mamba2] Performing single inference step.")

        # in_proj計算
        if self.use_jax:
            zxbcdt_jax = jnp.dot(u_jax.squeeze(1), torch_to_jax(self.in_proj.weight).T)  # JAXで線形変換
            zxbcdt = jax_to_torch(zxbcdt_jax)  # 結果をPyTorchのテンソルに戻す
        else:
            zxbcdt = self.in_proj(u.squeeze(1))  # PyTorchの場合

        print(f"  [Mamba2] in_proj (step) output shape: {zxbcdt.shape}")
        print(f"  [Mamba2] in_proj (step) output sample values: {zxbcdt.flatten()[:5]}")

        # テンソルの分割をsplit_tensorを使用して切り替える
        split_sizes = [self.args.d_inner, self.args.d_inner + 2 * self.args.d_state, self.args.nheads]
        z, xBC, dt = split_tensor(zxbcdt, split_sizes, dim=-1, use_jax=self.use_jax)
        print(f"  [Mamba2] Split shapes (step) -> z: {z.shape}, xBC: {xBC.shape}, dt: {dt.shape}")
        print(f"  [Mamba2] z (step) sample values: {z.flatten()[:5]}")
        print(f"  [Mamba2] xBC (step) sample values: {xBC.flatten()[:5]}")
        print(f"  [Mamba2] dt (step) sample values: {dt.flatten()[:5]}")

        # Advance convolution input
        h.conv_state.copy_(torch.roll(h.conv_state, shifts=-1, dims=-1))
        h.conv_state[:, :, -1] = xBC
        print(f"  [Mamba2] conv_state updated shape: {h.conv_state.shape}")
        print(f"  [Mamba2] conv_state updated sample values: {h.conv_state.flatten()[:5]}")

        ################
        # Convolution step
        ################
        if self.use_jax:
            print("[Mamba2] Using JAX for convolution and rearrange")
            conv_weight_jax = torch_to_jax(self.conv1d.weight)
            h_conv_state_jax = torch_to_jax(h.conv_state)
            conv_weight_jax = jnp.squeeze(conv_weight_jax, axis=1)  # "d 1 w -> d w"
            xBC_jax = jnp.sum(h_conv_state_jax * conv_weight_jax, axis=-1)
            xBC = jax_to_torch(xBC_jax)
        else:
            print("[Mamba2] Using PyTorch for convolution and rearrange")
            xBC = torch.sum(
                h.conv_state * rearrange(self.conv1d.weight, "d 1 w -> d w"), dim=-1
            )
        print(f"  [Mamba2] xBC after convolution sum shape: {xBC.shape}")
        print(f"  [Mamba2] xBC after convolution sum sample values: {xBC.flatten()[:5]}")

        if self.conv1d.bias is not None:
            xBC += self.conv1d.bias
            print(f"  [Mamba2] Added conv1d bias shape: {xBC.shape}")
            print(f"  [Mamba2] xBC after adding bias sample values: {xBC.flatten()[:5]}")

        xBC = silu(xBC, use_jax=self.use_jax)
        print(f"  [Mamba2] xBC after silu shape: {xBC.shape}")
        print(f"  [Mamba2] xBC after silu sample values: {xBC.flatten()[:5]}")

        split_sizes = [self.args.d_inner, self.args.d_state, self.args.d_state]
        x, B, C = split_tensor(xBC, split_sizes, dim=-1, use_jax=self.use_jax)
        print(f"  [Mamba2] Split xBC -> x: {x.shape}, B: {B.shape}, C: {C.shape}")
        print(f"  [Mamba2] x (step) sample values: {x.flatten()[:5]}")
        print(f"  [Mamba2] B (step) sample values: {B.flatten()[:5]}")
        print(f"  [Mamba2] C (step) sample values: {C.flatten()[:5]}")

        # Aの計算
        if self.use_jax:
            A_jax = -jnp.exp(A_log_jax)  # JAXでのA計算
            A = jax_to_torch(A_jax)  # PyTorchに戻す
        else:
            A = -torch.exp(self.A_log)  # PyTorchの場合
        print(f"  [Mamba2] A shape: {A.shape}")
        print(f"  [Mamba2] A sample values: {A.flatten()[:5]}")

        ###############
        # SSM step
        ###############
        if self.use_jax:
            dt_jax = jax.nn.softplus(torch_to_jax(dt) + dt_bias_jax)  # JAXでsoftplus
            dt = jax_to_torch(dt_jax)  # 結果をPyTorchに戻す
        else:
            dt = F.softplus(dt + self.dt_bias)  # PyTorchの場合
        print(f"  [Mamba2] dt after softplus (step) shape: {dt.shape}")
        print(f"  [Mamba2] dt after softplus (step) sample values: {dt.flatten()[:5]}")

        # dAの計算
        if self.use_jax:
            dA_jax = jnp.exp(torch_to_jax(dt) * torch_to_jax(A))  # JAXでの計算
            dA = jax_to_torch(dA_jax)  # PyTorchに戻す
        else:
            dA = torch.exp(dt * A)  # PyTorchの場合
        print(f"  [Mamba2] dA shape: {dA.shape}")
        print(f"  [Mamba2] dA sample values: {dA.flatten()[:5]}")

        # rearrange 実行前の x の形状を確認
        print(f"[Mamba2] Before rearrange, x shape: {x.shape}")
        if self.use_jax:
            print("[Mamba2] Using JAX for rearrange")

            # JAXでの次元変換 (reshapeを使用)
            x_jax = torch_to_jax(x)

            # 入力の次元数に基づいて、期待されるサイズを計算 (nheads * headdim)
            expected_size = self.args.nheads * self.args.headdim
            actual_size = x_jax.shape[-1]

            if actual_size != expected_size:
                raise ValueError(f"Shape mismatch: expected {expected_size}, but got {actual_size}")

            # 次元を (batch, nheads, headdim) に変換
            x_jax = jnp.reshape(x_jax, (x_jax.shape[0], self.args.nheads, self.args.headdim))
            print(f"[Mamba2] After JAX rearrange, x_jax shape: {x_jax.shape}")
            x = jax_to_torch(x_jax)

        else:
            print("[Mamba2] Using PyTorch for rearrange")

            # PyTorchでのrearrange (einops.rearrangeを使用)
            #x = rearrange(x, "b l (h p) -> b l h p", p=self.args.headdim)
            x = rearrange(x, "b (h p) -> b h p", p=self.args.headdim)
            print(f"[Mamba2] After PyTorch rearrange, x shape: {x.shape}")
        print(f"  [Mamba2] x after rearrange (step) shape: {x.shape}")
        print(f"  [Mamba2] x after rearrange (step) sample values: {x.flatten()[:5]}")

        # dBxの計算
        if self.use_jax:
            dBx_jax = jnp.einsum("bh, bn, bhp -> bhpn", torch_to_jax(dt), torch_to_jax(B), torch_to_jax(x))  # JAXでの計算
            dBx = jax_to_torch(dBx_jax)  # PyTorchに戻す
        else:
            dBx = torch.einsum("bh, bn, bhp -> bhpn", dt, B, x)  # PyTorchの場合
        print(f"  [Mamba2] dBx shape: {dBx.shape}")
        print(f"  [Mamba2] dBx sample values: {dBx.flatten()[:5]}")
        print(f"  [Mamba2] dBx shape: {dBx.shape}")
        print(f"  [Mamba2] dBx sample values: {dBx.flatten()[:5]}")

        if self.use_jax:
            print("[Mamba2] Using JAX for ssm_state, einsum, and rearrange computation")

            # JAXでのssm_state更新
            ssm_state_jax = torch_to_jax(h.ssm_state)
            dA_jax = torch_to_jax(dA)
            dBx_jax = torch_to_jax(dBx)
            updated_ssm_state_jax = ssm_state_jax * jnp.reshape(dA_jax, (dA_jax.shape[0], dA_jax.shape[1], 1, 1)) + dBx_jax

            # ここでh.ssm_stateに直接代入するのではなく、新しいhを生成
            new_ssm_state = jax_to_torch(updated_ssm_state_jax)
            h = InferenceCache(conv_state=h.conv_state, ssm_state=new_ssm_state)  # 新しいhオブジェクトを生成

            print(f"  [Mamba2] ssm_state updated shape: {h.ssm_state.shape}")
            print(f"  [Mamba2] ssm_state updated sample values: {h.ssm_state.flatten()[:5]}")

            # JAXでのeinsum
            C_jax = torch_to_jax(C)
            y_jax = jnp.einsum("bhpn, bn -> bhp", updated_ssm_state_jax, C_jax)
            y = jax_to_torch(y_jax)
            print(f"  [Mamba2] y after einsum shape: {y.shape}")
            print(f"  [Mamba2] y after einsum sample values: {y.flatten()[:5]}")

            # JAXでのrearrange
            D_jax = torch_to_jax(self.D)
            D_jax_reshaped = jnp.reshape(D_jax, (D_jax.shape[0], 1))
            x_jax = torch_to_jax(x)
            y_jax = y_jax + D_jax_reshaped * x_jax
            y = jax_to_torch(y_jax)
            print(f"  [Mamba2] y after adding D scaling (step) shape: {y.shape}")
            print(f"  [Mamba2] y after adding D scaling (step) sample values: {y.flatten()[:5]}")

            y_jax = jnp.reshape(y_jax, (y_jax.shape[0], -1))  # reshape equivalent to rearrange
            y = jax_to_torch(y_jax)
            print(f"  [Mamba2] y after rearrange to (b, d_inner) shape: {y.shape}")
            print(f"  [Mamba2] y after rearrange to (b, d_inner) sample values: {y.flatten()[:5]}")

        else:
            # PyTorchの操作をそのまま使用
            h.ssm_state.copy_(h.ssm_state * rearrange(dA, "b h -> b h 1 1") + dBx)
            print(f"  [Mamba2] ssm_state updated shape: {h.ssm_state.shape}")
            print(f"  [Mamba2] ssm_state updated sample values: {h.ssm_state.flatten()[:5]}")

            y = torch.einsum("bhpn, bn -> bhp", h.ssm_state, C)
            print(f"  [Mamba2] y after einsum shape: {y.shape}")
            print(f"  [Mamba2] y after einsum sample values: {y.flatten()[:5]}")

            y = y + rearrange(self.D, "h -> h 1") * x
            print(f"  [Mamba2] y after adding D scaling (step) shape: {y.shape}")
            print(f"  [Mamba2] y after adding D scaling (step) sample values: {y.flatten()[:5]}")

            y = rearrange(y, "b h p -> b (h p)")
            print(f"  [Mamba2] y after rearrange to (b, d_inner) shape: {y.shape}")
            print(f"  [Mamba2] y after rearrange to (b, d_inner) sample values: {y.flatten()[:5]}")

        # RMSNormの処理は既存のコードをそのまま使用
        y = self.norm(y, z)
        print(f"  [Mamba2] y after RMSNorm (step) shape: {y.shape}")
        print(f"  [Mamba2] y after RMSNorm (step) sample values: {y.flatten()[:5]}")

        if self.use_jax:
            y_jax = jnp.dot(torch_to_jax(y), torch_to_jax(self.out_proj.weight).T)  # JAXでの線形変換
            y = jax_to_torch(y_jax)  # 結果をPyTorchに戻す
        else:
            y = self.out_proj(y)
        print(f"  [Mamba2] y after out_proj (step) shape: {y.shape}")
        print(f"  [Mamba2] y after out_proj (step) sample values: {y.flatten()[:5]}")

        return y.unsqueeze(1), h



#def segsum(x: Tensor, device: Device = None, use_jax: bool = False) -> Tensor:
def segsum(x: Tensor, device: Device = None, use_jax: bool = True) -> Tensor:
    """Stable segment sum calculation with optional JAX support.

    exp(segsum(A)) produces a 1-semiseparable matrix, which is equivalent to a scalar SSM.

    Source: https://github.com/state-spaces/mamba/blob/219f03c840d5a44e7d42e4e728134834fddccf45/mamba_ssm/modules/ssd_minimal.py#L23-L32
    """
    T = x.size(-1)  # 時間次元のサイズ

    if use_jax:
        x_jax = torch_to_jax(x)

        # JAXでrepeat相当の処理
        x_jax = jnp.tile(x_jax[..., None], (1, 1, T))
        # 下三角行列のマスクをJAXで作成
        mask_jax = jnp.tril(jnp.ones((T, T), dtype=bool), k=-1)
        # JAXでmasked_fill相当の処理
        x_jax = jnp.where(mask_jax, x_jax, 0)
        # JAXで累積和 (cumsum)
        x_segsum_jax = jnp.cumsum(x_jax, axis=-2)
        # 再度マスクを適用 (下三角部分のみ残す)
        mask_jax = jnp.tril(jnp.ones((T, T), dtype=bool), k=0)
        x_segsum_jax = jnp.where(mask_jax, x_segsum_jax, -jnp.inf)

        x_segsum = jax_to_torch(x_segsum_jax)
    else:
        # PyTorchでの計算
        x = repeat(x, "... d -> ... d e", e=T)
        mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=device), diagonal=-1)
        x = x.masked_fill(~mask, 0)
        x_segsum = torch.cumsum(x, dim=-2)
        mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=device), diagonal=0)
        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)

    return x_segsum


#def ssd(x, A, B, C, chunk_size, initial_states=None, device: Device = None, use_jax=False):
def ssd(x, A, B, C, chunk_size, initial_states=None, device: Device = None, use_jax=True):
    """Structured State Space Duality (SSD) - the core of Mamba-2

    This is almost the exact same minimal SSD code from the blog post.

    Arguments
        x: (batch, seqlen, n_heads, d_head)
        A: (batch, seqlen, n_heads)
        B: (batch, seqlen, n_heads, d_state)
        C: (batch, seqlen, n_heads, d_state)

    Return
        y: (batch, seqlen, n_heads, d_head)
    """
    print("[ssd] Starting SSD computation...")
    assert x.shape[1] % chunk_size == 0, "Sequence length must be divisible by chunk_size."

    if use_jax:
        # JAXに変換
        x_jax, A_jax, B_jax, C_jax = [torch_to_jax(t) for t in (x, A, B, C)]

        # Rearrange into chunks (JAX equivalent of rearrange)
        print("[ssd] Rearranging tensors into chunks (JAX).")
        x_jax, A_jax, B_jax, C_jax = [
            jnp.reshape(t, (t.shape[0], t.shape[1] // chunk_size, chunk_size, *t.shape[2:]))
            for t in (x_jax, A_jax, B_jax, C_jax)
        ]
        print(f"  [ssd] After chunking shapes -> x: {x_jax.shape}, A: {A_jax.shape}, B: {B_jax.shape}, C: {C_jax.shape}")

        # Rearrange A (batch, seqlen, n_heads) -> (batch, n_heads, seqlen, chunk_size) for JAX
        A_jax = jnp.transpose(A_jax, (0, 3, 1, 2))
        A_cumsum_jax = jnp.cumsum(A_jax, axis=-1)

        # Compute the output for each intra-chunk (diagonal blocks)
        L_jax = jnp.exp(torch_to_jax(segsum(jax_to_torch(A_jax), device=device)))
        Y_diag_jax = jnp.einsum("bclhn, bcshn, bhcls, bcshp -> bclhp", C_jax, B_jax, L_jax, x_jax)

        # Compute the state for each intra-chunk
        decay_states_jax = jnp.exp(A_cumsum_jax[..., -1:] - A_cumsum_jax)
        states_jax = jnp.einsum("bclhn, bhcl, bclhp -> bchpn", B_jax, decay_states_jax, x_jax)

        # Compute the inter-chunk SSM recurrence
        if initial_states is None:
            initial_states_jax = jnp.zeros_like(states_jax[:, :1])
        else:
            initial_states_jax = torch_to_jax(initial_states)

        states_jax = jnp.concatenate([initial_states_jax, states_jax], axis=1)
        decay_chunk_jax = jnp.exp(torch_to_jax(segsum(jax_to_torch(jnp.pad(A_cumsum_jax[..., -1:], ((0, 0), (0, 0), (0, 0), (1, 0)))), device=device)))
        new_states_jax = jnp.einsum("bhzc, bchpn -> bzhpn", decay_chunk_jax, states_jax)
        states_jax, final_state_jax = new_states_jax[:, :-1], new_states_jax[:, -1]

        # Compute state -> output conversion per chunk
        state_decay_out_jax = jnp.exp(A_cumsum_jax)
        Y_off_jax = jnp.einsum("bclhn, bchpn, bhcl -> bclhp", C_jax, states_jax, state_decay_out_jax)

        # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)
        Y_jax = Y_diag_jax + Y_off_jax
        Y_jax = jnp.reshape(Y_jax, (Y_jax.shape[0], Y_jax.shape[1] * Y_jax.shape[2], *Y_jax.shape[3:]))

        # JAXの結果をPyTorchに変換して返す
        Y = jax_to_torch(Y_jax)
        final_state = jax_to_torch(final_state_jax)

    else:
        # PyTorchの処理
        print("[ssd] Rearranging tensors into chunks.")
        x, A, B, C = [
            rearrange(m, "b (c l) ... -> b c l ...", l=chunk_size) for m in (x, A, B, C)
        ]
        print(f"  [ssd] After chunking shapes -> x: {x.shape}, A: {A.shape}, B: {B.shape}, C: {C.shape}")

        A = rearrange(A, "b c l h -> b h c l")
        A_cumsum = torch.cumsum(A, dim=-1)

        # Compute the output for each intra-chunk (diagonal blocks)
        L = torch.exp(segsum(A, device=device))
        Y_diag = torch.einsum("bclhn, bcshn, bhcls, bcshp -> bclhp", C, B, L, x)

        # Compute the state for each intra-chunk
        decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)
        states = torch.einsum("bclhn, bhcl, bclhp -> bchpn", B, decay_states, x)

        # Compute the inter-chunk SSM recurrence
        if initial_states is None:
            initial_states = torch.zeros_like(states[:, :1])
        states = torch.cat([initial_states, states], dim=1)
        decay_chunk = torch.exp(segsum(F.pad(A_cumsum[:, :, :, -1:], (1, 0)), device=device))
        new_states = torch.einsum("bhzc, bchpn -> bzhpn", decay_chunk, states)
        states, final_state = new_states[:, :-1], new_states[:, -1]

        # Compute state -> output conversion per chunk
        state_decay_out = torch.exp(A_cumsum)
        Y_off = torch.einsum("bclhn, bchpn, bhcl -> bclhp", C, states, state_decay_out)

        # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)
        Y = rearrange(Y_diag + Y_off, "b c l h p -> b (c l) h p")

    print(f"[ssd] Completed SSD computation. Y shape: {Y.shape}, final_state shape: {final_state.shape}")
    return Y, final_state


class RMSNorm(torch.nn.Module):
    #def __init__(self, d: int, eps: float = 1e-5, device=None, use_jax=False):
    def __init__(self, d: int, eps: float = 1e-5, device=None, use_jax=True):
        """Gated Root Mean Square Layer Normalization

        Paper: https://arxiv.org/abs/1910.07467
        """
        super().__init__()
        self.eps = eps
        self.weight = torch.nn.Parameter(torch.ones(d, device=device))
        self.use_jax = use_jax  # JAXを使うかどうかを選択
        print(f"[RMSNorm] Initialized RMSNorm with d={d}, eps={eps}")
        print(f"[RMSNorm] weight shape: {self.weight.shape}")
        print(f"[RMSNorm] weight sample values: {self.weight.flatten()[:5]}")

    def forward(self, x, z=None):
        if z is not None:
            # SiLUによるゲーティングを適用
            x = x * silu(z, self.use_jax)
            print(f"[RMSNorm] x after gated scaling shape: {x.shape}")
            print(f"[RMSNorm] x after gated scaling sample values: {x.flatten()[:5]}")

        if self.use_jax:
            # JAXでのRMSNorm計算
            print("[RMSNorm] Using JAX for the RMSNorm computation")
            x_jax = torch_to_jax(x)
            mean_sq_jax = jnp.mean(jnp.square(x_jax), axis=-1, keepdims=True)
            rsqrt_jax = jnp.sqrt(1 / (mean_sq_jax + self.eps))
            x_normalized_jax = x_jax * rsqrt_jax * torch_to_jax(self.weight)
            x_normalized = jax_to_torch(x_normalized_jax)

        else:
            # PyTorchでのRMSNorm計算
            print("[RMSNorm] Using PyTorch for the RMSNorm computation")
            mean_sq = x.pow(2).mean(-1, keepdim=True)
            print(f"[RMSNorm] Mean squared value shape: {mean_sq.shape}")
            print(f"[RMSNorm] mean_sq sample values: {mean_sq.flatten()[:5]}")
            rsqrt = torch.rsqrt(mean_sq + self.eps)
            print(f"[RMSNorm] rsqrt shape: {rsqrt.shape}")
            print(f"[RMSNorm] rsqrt sample values: {rsqrt.flatten()[:5]}")
            x_normalized = x * rsqrt * self.weight

        print(f"[RMSNorm] x_normalized shape: {x_normalized.shape}")
        print(f"[RMSNorm] x_normalized sample values: {x_normalized.flatten()[:5]}")
        return x_normalized

#def silu(x, use_jax=False):
def silu(x, use_jax=True):
    """Applies the Sigmoid Linear Unit (SiLU), element-wise.

    This function supports both JAX and PyTorch computations depending on the use_jax flag.
    """
    if use_jax:
        print("[SiLU] Using JAX for SiLU computation")
        x_jax = torch_to_jax(x)
        result_jax = x_jax * jax.nn.sigmoid(x_jax)
        return jax_to_torch(result_jax)
    else:
        print("[SiLU] Using PyTorch for SiLU computation")
        return x * torch.sigmoid(x)
