"""
mamba2-minimal
==============

A minimal, single-file implementation of the Mamba-2 model in PyTorch.

> **Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality**
> Authors: Tri Dao, Albert Gu
> Paper: https://arxiv.org/abs/2405.21060
"""

import json
from dataclasses import dataclass
from typing import Iterable, NamedTuple, TypeAlias, cast

import torch
import torch.nn.functional as F
from einops import rearrange, repeat
from torch import LongTensor, Tensor, nn

Device: TypeAlias = str | torch.device | None


import jax
import jax.numpy as jnp
import numpy as np

def torch_to_jax(x):
    return jnp.array(x.detach().cpu().numpy())
def jax_to_torch(x):
    return torch.tensor(np.array(x), dtype=torch.float32)

def split_tensor(x, split_sizes, dim, use_jax=True):
    """
    Splits the tensor x into multiple tensors based on split_sizes along dimension dim.
    Uses JAX for splitting if use_jax=True, otherwise uses PyTorch.

    Args:
        x (Tensor or jnp.ndarray): The input tensor to split.
        split_sizes (list of int): The sizes to split the tensor into.
        dim (int): The dimension along which to split.
        use_jax (bool): Whether to use JAX for splitting.

    Returns:
        list of Tensor or jnp.ndarray: The list of split tensors.
    """
    splits = []
    start = 0
    x_jax = x
    for size in split_sizes:
        end = start + size
        # JAXでインデックスを生成
        indices = jnp.arange(start, end)
        # JAXのtakeを使用して分割
        split_jax = jnp.take(x_jax, indices=indices, axis=dim)
        split = split_jax
        splits.append(split)
        start = end
    return splits


@dataclass
class Mamba2Config:
    d_model: int  # model dimension (D)
    n_layer: int = 24  # number of Mamba-2 layers in the language model
    d_state: int = 128  # state dimension (N)
    d_conv: int = 4  # convolution kernel size
    expand: int = 2  # expansion factor (E)
    headdim: int = 64  # head dimension (P)
    chunk_size: int = 64  # matrix partition size (Q)
    vocab_size: int = 50277
    pad_vocab_size_multiple: int = 16

    def __post_init__(self):
        self.d_inner = self.expand * self.d_model
        assert self.d_inner % self.headdim == 0, "d_inner must be divisible by headdim"
        self.nheads = self.d_inner // self.headdim
        if self.vocab_size % self.pad_vocab_size_multiple != 0:
            self.vocab_size += (
                self.pad_vocab_size_multiple
                - self.vocab_size % self.pad_vocab_size_multiple
            )
        print(f"[Config] Initialized Mamba2Config with d_model={self.d_model}, "
              f"n_layer={self.n_layer}, d_state={self.d_state}, d_conv={self.d_conv}, "
              f"expand={self.expand}, headdim={self.headdim}, chunk_size={self.chunk_size}, "
              f"vocab_size={self.vocab_size}, pad_vocab_size_multiple={self.pad_vocab_size_multiple}")
        print(f"[Config] Computed d_inner={self.d_inner}, nheads={self.nheads}")


@dataclass
class InferenceCache:
    conv_state: jnp.ndarray  # (batch, d_inner + 2 * d_state, d_conv)
    ssm_state: jnp.ndarray   # (batch, nheads, headdim, d_state)

    @staticmethod
    #def alloc(batch_size: int, args: 'Mamba2Config') -> 'InferenceCache':
    def alloc(batch_size: int, args: Mamba2Config, device: Device = None) -> 'InferenceCache':
        print(f"[InferenceCache] Allocating InferenceCache for batch size: {batch_size}")

        conv_state = jnp.zeros(
            (batch_size, args.d_inner + 2 * args.d_state, args.d_conv)
        )
        print(f"[InferenceCache] conv_state shape: {conv_state.shape}")
        print(f"[InferenceCache] conv_state sample values: {conv_state.flatten()[:5]}")

        ssm_state = jnp.zeros(
            (batch_size, args.nheads, args.headdim, args.d_state)
        )
        print(f"[InferenceCache] ssm_state shape: {ssm_state.shape}")
        print(f"[InferenceCache] ssm_state sample values: {ssm_state.flatten()[:5]}")

        return InferenceCache(conv_state, ssm_state)


class Mamba2LMHeadModel(nn.Module):
    def __init__(self, args: Mamba2Config, device: Device = None, use_jax = True):
        super().__init__()
        self.args = args
        self.device = device
        self.use_jax = use_jax

        self.backbone = nn.ModuleDict(
            dict(
                embedding=nn.Embedding(args.vocab_size, args.d_model, device=device),
                layers=nn.ModuleList(
                    [
                        nn.ModuleDict(
                            dict(
                                mixer=Mamba2(args, device=device),
                                norm=RMSNorm(args.d_model, device=device),
                            )
                        )
                        for _ in range(args.n_layer)
                    ]
                ),
                norm_f=RMSNorm(args.d_model, device=device),
            )
        )
        self.lm_head = nn.Linear(
            args.d_model, args.vocab_size, bias=False, device=device
        )
        self.lm_head.weight = self.backbone.embedding.weight

    @staticmethod
    def from_pretrained(huggingface_model_id: str, device: Device = None):
        from transformers.utils import CONFIG_NAME, WEIGHTS_NAME
        from transformers.utils.hub import cached_file

        print(f"[Mamba2LMHeadModel] Loading pre-trained model from HuggingFace ID: {huggingface_model_id}")
        config_path = cached_file(huggingface_model_id, CONFIG_NAME)
        assert config_path, "Failed to get HuggingFace config file"
        state_dict_path = cached_file(huggingface_model_id, WEIGHTS_NAME)
        assert state_dict_path, "Failed to get HuggingFace state dict file"

        config = json.load(open(config_path))
        args = Mamba2Config(
            d_model=config["d_model"],
            n_layer=config["n_layer"],
            vocab_size=config["vocab_size"],
            pad_vocab_size_multiple=config.get("pad_vocab_size_multiple", 16),
        )

        map_location = "cpu" if device is None else device
        state_dict = torch.load(
            state_dict_path, map_location=map_location, weights_only=True
        )
        print("[Mamba2LMHeadModel] Loaded state dict.")
        model = Mamba2LMHeadModel(args, device=device)
        model.load_state_dict(state_dict)
        model.eval()
        print("[Mamba2LMHeadModel] Model loaded and set to evaluation mode.")

        # Print shapes of some key parameters for debugging
        print("[Mamba2LMHeadModel] Loaded model parameters:")
        for name, param in model.named_parameters():
            print(f"  {name}: {param.shape}")
            # Print sample values
            if param.numel() > 0:
                print(f"    Sample values from {name}: {param.flatten()[:5]}")
            break  # Remove this break to see more parameters

        return model

    def forward(
        self, input_ids: LongTensor, h: list[InferenceCache] | list[None] | None = None
    ) -> tuple[LongTensor, list[InferenceCache]]:
        """
        Arguments
            input_ids: (batch, seqlen) tokens from EleutherAI/gpt-neox-20b tokenizer
            h: hidden states for inference step. If present the constant-time
               (wrt sequence length) inference path will be taken, input_ids
               should have shape (batch, 1) containing the next batch of prompt
               token.

        Return (logits, h)
            logits: (batch, seqlen, vocab_size)
            h: updated inference cache after processing input_ids
        """
        print(f"[Mamba2LMHeadModel] Forward pass input_ids shape: {input_ids.shape}")
        print(f"[Mamba2LMHeadModel] input_ids sample values: {input_ids.flatten()[:5]}")
        seqlen = input_ids.shape[1]

        if h is None:
            h = [None for _ in range(self.args.n_layer)]
            print("[Mamba2LMHeadModel] Initialized hidden states.")

        x = self.backbone.embedding(input_ids)
        print(f"[Mamba2LMHeadModel] Embedding output shape: {x.shape}")
        print(f"[Mamba2LMHeadModel] Embedding sample values: {x.flatten()[:5]}")

        for i, layer in enumerate(self.backbone.layers):
            print(f"[Mamba2LMHeadModel] Processing layer {i + 1}/{self.args.n_layer}")
            x_jax = torch_to_jax(x)
            y, h[i] = layer.mixer(
                jax_to_torch(layer.norm(x_jax)),
                h[i])
            print(f"  [Layer {i + 1}] Output shape after mixer: {y.shape}")
            print(f"  [Layer {i + 1}] Output sample values after mixer: {y.flatten()[:5]}")
            x = y + x
            print(f"  [Layer {i + 1}] Residual connection shape: {x.shape}")
            print(f"  [Layer {i + 1}] Residual connection sample values: {x.flatten()[:5]}")

        x_jax = torch_to_jax(x)
        x_jax = self.backbone.norm_f(x_jax)
        x = jax_to_torch(x_jax)
        print(f"[Mamba2LMHeadModel] Final backbone norm output shape: {x.shape}")
        print(f"[Mamba2LMHeadModel] Final backbone norm output sample values: {x.flatten()[:5]}")
        if self.use_jax:
            print("[Mamba2LMHeadModel] Using JAX for lm_head")
            lm_head_weight_jax = torch_to_jax(self.lm_head.weight)
            x_jax = torch_to_jax(x)
            logits_jax = jnp.dot(x_jax, lm_head_weight_jax.T)  # lm_headの重みを使ったドット積
            logits = jax_to_torch(logits_jax)
        else:
            print("[Mamba2LMHeadModel] Using PyTorch for lm_head")
            logits = self.lm_head(x)
        print(f"[Mamba2LMHeadModel] Logits shape: {logits.shape}")
        print(f"[Mamba2LMHeadModel] Logits sample values: {logits.flatten()[:5]}")

        return logits[:, :seqlen], cast(list[InferenceCache], h)

    def generate(
        self,
        input_ids: LongTensor,
        max_new_length: int = 20,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 1.0,
        eos_token_id: int = 0,
    ) -> Iterable[tuple[int, list[InferenceCache]]]:
        prefix, tokens = input_ids[:-1], input_ids[-1:].unsqueeze(0)

        # Process prompt
        # The input sequence to forward (non-inference path) must have length multiple that of chunk_size.
        # We split out excess tokens so that n_chunked tokens can be processed by one forward call and
        # process the rest in multiple inference steps.
        n_chunked = (prefix.shape[0] // self.args.chunk_size) * self.args.chunk_size
        if n_chunked > 0:
            _, h = self(prefix[:n_chunked].unsqueeze(0), None)
        else:
            h = [
                #InferenceCache.alloc(1, self.args, device=self.device)
                InferenceCache.alloc(1, self.args)
                for _ in range(self.args.n_layer)
            ]
        for i in range(n_chunked, prefix.shape[0]):
            _, h = self(prefix[i : i + 1].unsqueeze(0), h)

        # Generate
        for _ in range(max_new_length):
            with torch.no_grad():
                out, h = self(tokens, h)
            logits = out[0, -1]

            # Tempreture scaling の適用
            if temperature != 1.0:
                logits = logits / temperature

            if self.use_jax:
                logits_jax = torch_to_jax(logits)
                probs_jax = jax.nn.softmax(logits_jax)
                probs = jax_to_torch(probs_jax)
            else:
                probs = F.softmax(logits, dim=-1)

            # 確率を基にtop_k, top_pを適用する
            if top_k > 0:
                indices_to_remove = probs < torch.topk(probs, k=top_k)[0][-1]
                probs[indices_to_remove] = 0  # 削除対象の確率を0に設定
            if top_p < 1.0:
                sorted_probs, sorted_indices = torch.sort(probs, descending=True)

                if self.use_jax:
                    sorted_probs_jax = torch_to_jax(sorted_probs)
                    cum_probs_jax = jnp.cumsum(sorted_probs_jax)
                    cum_probs = jax_to_torch(cum_probs_jax)
                else:
                    cum_probs = torch.cumsum(sorted_probs, dim=-1)

                sorted_indices_to_remove = cum_probs > top_p
                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                sorted_indices_to_remove[0] = False
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                probs[indices_to_remove] = 0  # 削除対象の確率を0に設定

            # ここで次のトークンを確率に基づいてサンプリング
            next_token = torch.multinomial(probs, num_samples=1)

            if next_token.item() == eos_token_id:
                return

            tokens = next_token.unsqueeze(0)
            yield cast(int, next_token.item()), h


class Mamba2(nn.Module):
    def __init__(self, args: Mamba2Config, device: Device = None, use_jax=True):
        super().__init__()
        self.args = args
        self.device = device
        self.use_jax = use_jax

        print("[Mamba2] Initializing Mamba2 mixer...")
        d_in_proj = 2 * args.d_inner + 2 * args.d_state + args.nheads
        print(f"  [Mamba2] d_in_proj: {d_in_proj}")
        self.in_proj = nn.Linear(args.d_model, d_in_proj, bias=False, device=device)
        print(f"  [Mamba2] Initialized in_proj with input_dim={args.d_model}, output_dim={d_in_proj}")
        print(f"  [Mamba2] in_proj.weight shape: {self.in_proj.weight.shape}")
        print(f"  [Mamba2] in_proj.weight sample values: {self.in_proj.weight.flatten()[:5]}")

        conv_dim = args.d_inner + 2 * args.d_state
        self.conv1d = nn.Conv1d(
            in_channels=conv_dim,
            out_channels=conv_dim,
            kernel_size=args.d_conv,
            groups=conv_dim,
            padding=args.d_conv - 1,
            device=device,
        )
        print(f"  [Mamba2] Initialized conv1d with in_channels={conv_dim}, "
              f"out_channels={conv_dim}, kernel_size={args.d_conv}")
        print(f"  [Mamba2] conv1d.weight shape: {self.conv1d.weight.shape}")
        print(f"  [Mamba2] conv1d.weight sample values: {self.conv1d.weight.flatten()[:5]}")

        self.dt_bias = nn.Parameter(torch.empty(args.nheads, device=device))
        print(f"  [Mamba2] Initialized dt_bias with shape: {self.dt_bias.shape}")
        print(f"  [Mamba2] dt_bias sample values: {self.dt_bias.flatten()[:5]}")
        self.A_log = nn.Parameter(torch.empty(args.nheads, device=device))
        print(f"  [Mamba2] Initialized A_log with shape: {self.A_log.shape}")
        print(f"  [Mamba2] A_log sample values: {self.A_log.flatten()[:5]}")
        self.D = nn.Parameter(torch.empty(args.nheads, device=device))
        print(f"  [Mamba2] Initialized D with shape: {self.D.shape}")
        print(f"  [Mamba2] D sample values: {self.D.flatten()[:5]}")
        print("  [Mamba2] Initialized A_log and D parameters.")

        self.norm = RMSNorm(args.d_inner, device=device, use_jax=self.use_jax)
        print("  [Mamba2] Initialized RMSNorm.")

        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=False, device=device)
        print("  [Mamba2] Initialized out_proj.")
        print(f"  [Mamba2] out_proj.weight shape: {self.out_proj.weight.shape}")
        print(f"  [Mamba2] out_proj.weight sample values: {self.out_proj.weight.flatten()[:5]}")

        # Initialize parameters
        self._init_parameters()

    def _init_parameters(self):
        print("[Mamba2] Initializing parameters with normal distribution.")
        nn.init.normal_(self.dt_bias, mean=0.0, std=0.02)
        print(f"  [Mamba2] dt_bias after init: {self.dt_bias.flatten()[:5]}")
        nn.init.normal_(self.A_log, mean=0.0, std=0.02)
        print(f"  [Mamba2] A_log after init: {self.A_log.flatten()[:5]}")
        nn.init.normal_(self.D, mean=0.0, std=0.02)
        print(f"  [Mamba2] D after init: {self.D.flatten()[:5]}")
        nn.init.normal_(self.in_proj.weight, mean=0.0, std=0.02)
        print(f"  [Mamba2] in_proj.weight after init: {self.in_proj.weight.flatten()[:5]}")
        nn.init.normal_(self.conv1d.weight, mean=0.0, std=0.02)
        print(f"  [Mamba2] conv1d.weight after init: {self.conv1d.weight.flatten()[:5]}")
        nn.init.normal_(self.out_proj.weight, mean=0.0, std=0.02)
        print(f"  [Mamba2] out_proj.weight after init: {self.out_proj.weight.flatten()[:5]}")
        print("[Mamba2] Parameters initialized.")
    '''
    def __init__(self, args: Mamba2Config, device: Device = None, use_jax = True):
        super().__init__()
        self.args = args
        self.device = device
        self.use_jax = use_jax

        # Order: (z, x, B, C, dt)
        d_in_proj = 2 * args.d_inner + 2 * args.d_state + args.nheads
        self.in_proj = nn.Linear(args.d_model, d_in_proj, bias=False, device=device)

        conv_dim = args.d_inner + 2 * args.d_state
        self.conv1d = nn.Conv1d(
            in_channels=conv_dim,
            out_channels=conv_dim,
            kernel_size=args.d_conv,
            groups=conv_dim,
            padding=args.d_conv - 1,
            device=device,
        )

        self.dt_bias = nn.Parameter(torch.empty(args.nheads, device=device))
        self.A_log = nn.Parameter(torch.empty(args.nheads, device=device))
        self.D = nn.Parameter(torch.empty(args.nheads, device=device))
        self.norm = RMSNorm(args.d_inner, device=device)
        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=False, device=device)
    '''

    def forward(self, u: Tensor, h: InferenceCache | None = None):
        """
        Arguments
            u: (batch, seqlen, d_model) input. seqlen should be a multiple of chunk_size.
            h: hidden states for inference step. Initialized to 0s if not present.

        Return (y, h)
            y: (batch, seqlen, d_model) output
            h: updated inference cache after processing `u`
        """
        if h:
            return self.step(u, h)

        A = -torch.exp(self.A_log)  # (nheads,)
        zxbcdt = self.in_proj(u)  # (batch, seqlen, d_in_proj)
        z, xBC, dt = torch.split(
            zxbcdt,
            [
                self.args.d_inner,
                self.args.d_inner + 2 * self.args.d_state,
                self.args.nheads,
            ],
            dim=-1,
        )
        dt = F.softplus(dt + self.dt_bias)  # (batch, seqlen, nheads)

        # Pad or truncate xBC seqlen to d_conv
        conv_state = F.pad(
            rearrange(xBC, "b l d -> b d l"), (self.args.d_conv - u.shape[1], 0)
        )

        #xBC = silu(
        #    self.conv1d(xBC.transpose(1, 2)).transpose(1, 2)[:, : u.shape[1], :]
        #)  # (batch, seqlen, d_inner + 2 * d_state))
        xBC = jax_to_torch(silu(
            torch_to_jax(self.conv1d(xBC.transpose(1, 2)).transpose(1, 2)[:, : u.shape[1], :])
        ))  # (batch, seqlen, d_inner + 2 * d_state))
        x, B, C = torch.split(
            xBC, [self.args.d_inner, self.args.d_state, self.args.d_state], dim=-1
        )
        x = rearrange(x, "b l (h p) -> b l h p", p=self.args.headdim)
        y, ssm_state = ssd(
            x * dt.unsqueeze(-1),
            A * dt,
            rearrange(B, "b l n -> b l 1 n"),
            rearrange(C, "b l n -> b l 1 n"),
            self.args.chunk_size,
            device=self.device,
        )
        y = y + x * self.D.unsqueeze(-1)
        y = rearrange(y, "b l h p -> b l (h p)")
        #y = self.norm(y, z)
        y = jax_to_torch(self.norm(
            torch_to_jax(y),
            torch_to_jax(z)
        ))
        y = self.out_proj(y)

        #h = InferenceCache(conv_state, ssm_state)
        h = InferenceCache(torch_to_jax(conv_state), torch_to_jax(ssm_state))
        return y, h

    def step(self, u: Tensor, h: InferenceCache) -> tuple[Tensor, InferenceCache]:
        """Take a single inference step for the current input and hidden state

        Unlike attention-based models, RNN-based models (eg Mamba) do not need
        to look back at all the past tokens to generate a new token. Instead a
        hidden state (initialized to 0s initially) is updated for each input and
        passed to the next inference step. This means that the total inference
        time is linear with respect to the sequence length instead of quadratic
        in attention's case.

        Arguments
            u: (batch, 1, d_model)
            h: initial/running hidden state

        Return (y, h)
            y: (batch, 1, d_model)
            h: updated hidden state
        """
        print("[Mamba2] Performing full forward pass.")
        print("[Mamba2] Using JAX for Mamba2 computation")
        u_jax = torch_to_jax(u)  # PyTorchのテンソルをJAXのテンソルに変換
        A_log_jax = torch_to_jax(self.A_log)
        D_jax = torch_to_jax(self.D)
        dt_bias_jax = torch_to_jax(self.dt_bias)

        assert u.shape[1] == 1, "Only one token can be decoded per inference step"
        print("[Mamba2] Performing single inference step.")

        # in_proj計算
        zxbcdt = jnp.dot(u_jax.squeeze(1), torch_to_jax(self.in_proj.weight).T)  # JAXで線形変換

        print(f"  [Mamba2] in_proj (step) output shape: {zxbcdt.shape}")
        print(f"  [Mamba2] in_proj (step) output sample values: {zxbcdt.flatten()[:5]}")

        # テンソルの分割をsplit_tensorを使用して切り替える
        split_sizes = [self.args.d_inner, self.args.d_inner + 2 * self.args.d_state, self.args.nheads]
        z, xBC, dt = split_tensor(zxbcdt, split_sizes, dim=-1, use_jax=self.use_jax)
        print(f"  [Mamba2] Split shapes (step) -> z: {z.shape}, xBC: {xBC.shape}, dt: {dt.shape}")
        print(f"  [Mamba2] z (step) sample values: {z.flatten()[:5]}")
        print(f"  [Mamba2] xBC (step) sample values: {xBC.flatten()[:5]}")
        print(f"  [Mamba2] dt (step) sample values: {dt.flatten()[:5]}")

        # Advance convolution input
        rolled_conv_state = jnp.roll(h.conv_state, shift=-1, axis=-1)
        updated_conv_state = rolled_conv_state.at[:, :, -1].set(jax_to_torch(xBC))
        h = InferenceCache(conv_state=updated_conv_state, ssm_state=h.ssm_state)
        print(f"  [Mamba2] conv_state updated shape: {h.conv_state.shape}")
        print(f"  [Mamba2] conv_state updated sample values: {h.conv_state.flatten()[:5]}")

        ################
        # Convolution step
        ################
        print("[Mamba2] Using JAX for convolution and rearrange")
        conv_weight_jax = torch_to_jax(self.conv1d.weight)
        h_conv_state_jax = h.conv_state
        conv_weight_jax = jnp.squeeze(conv_weight_jax, axis=1)  # "d 1 w -> d w"
        xBC = jnp.sum(h_conv_state_jax * conv_weight_jax, axis=-1)
        print(f"  [Mamba2] xBC after convolution sum shape: {xBC.shape}")
        print(f"  [Mamba2] xBC after convolution sum sample values: {xBC.flatten()[:5]}")

        if self.conv1d.bias is not None:
            xBC += torch_to_jax(self.conv1d.bias)
            print(f"  [Mamba2] Added conv1d bias shape: {xBC.shape}")
            print(f"  [Mamba2] xBC after adding bias sample values: {xBC.flatten()[:5]}")

        xBC = silu(xBC, use_jax=self.use_jax)
        print(f"  [Mamba2] xBC after silu shape: {xBC.shape}")
        print(f"  [Mamba2] xBC after silu sample values: {xBC.flatten()[:5]}")

        split_sizes = [self.args.d_inner, self.args.d_state, self.args.d_state]
        x, B, C = split_tensor(xBC, split_sizes, dim=-1, use_jax=self.use_jax)
        x = jax_to_torch(x)
        B = jax_to_torch(B)
        C = jax_to_torch(C)
        print(f"  [Mamba2] Split xBC -> x: {x.shape}, B: {B.shape}, C: {C.shape}")
        print(f"  [Mamba2] x (step) sample values: {x.flatten()[:5]}")
        print(f"  [Mamba2] B (step) sample values: {B.flatten()[:5]}")
        print(f"  [Mamba2] C (step) sample values: {C.flatten()[:5]}")

        # Aの計算
        A = -jnp.exp(A_log_jax)  # JAXでのA計算
        print(f"  [Mamba2] A shape: {A.shape}")
        print(f"  [Mamba2] A sample values: {A.flatten()[:5]}")

        ###############
        # SSM step
        ###############
        dt = jax.nn.softplus(dt + dt_bias_jax)  # JAXでsoftplus
        print(f"  [Mamba2] dt after softplus (step) shape: {dt.shape}")
        print(f"  [Mamba2] dt after softplus (step) sample values: {dt.flatten()[:5]}")

        # dAの計算
        dA_jax = jnp.exp(dt * A)  # JAXでの計算
        dA = dA_jax
        print(f"  [Mamba2] dA shape: {dA.shape}")
        print(f"  [Mamba2] dA sample values: {dA.flatten()[:5]}")

        # rearrange 実行前の x の形状を確認
        print(f"[Mamba2] Before rearrange, x shape: {x.shape}")
        print("[Mamba2] Using JAX for rearrange")

        x_jax = torch_to_jax(x)
        expected_size = self.args.nheads * self.args.headdim
        actual_size = x_jax.shape[-1]
        if actual_size != expected_size:
            raise ValueError(f"Shape mismatch: expected {expected_size}, but got {actual_size}")
        x_jax = jnp.reshape(x_jax, (x_jax.shape[0], self.args.nheads, self.args.headdim))
        print(f"[Mamba2] After JAX rearrange, x_jax shape: {x_jax.shape}")
        x = jax_to_torch(x_jax)
        print(f"  [Mamba2] x after rearrange (step) shape: {x.shape}")
        print(f"  [Mamba2] x after rearrange (step) sample values: {x.flatten()[:5]}")

        dBx_jax = jnp.einsum("bh, bn, bhp -> bhpn", dt, torch_to_jax(B), torch_to_jax(x))  # JAXでの計算
        dBx = jax_to_torch(dBx_jax)  # PyTorchに戻す
        print(f"  [Mamba2] dBx shape: {dBx.shape}")
        print(f"  [Mamba2] dBx sample values: {dBx.flatten()[:5]}")
        print(f"  [Mamba2] dBx shape: {dBx.shape}")
        print(f"  [Mamba2] dBx sample values: {dBx.flatten()[:5]}")

        # JAXでのssm_state更新
        ssm_state_jax = h.ssm_state
        dBx_jax = torch_to_jax(dBx)
        updated_ssm_state_jax = ssm_state_jax * jnp.reshape(dA, (dA.shape[0], dA.shape[1], 1, 1)) + dBx_jax
        # ここでh.ssm_stateに直接代入するのではなく、新しいhを生成
        h = InferenceCache(conv_state=h.conv_state, ssm_state=updated_ssm_state_jax)  # 新しいhオブジェクトを生成

        print(f"  [Mamba2] ssm_state updated shape: {h.ssm_state.shape}")
        print(f"  [Mamba2] ssm_state updated sample values: {h.ssm_state.flatten()[:5]}")

        # JAXでのeinsum
        C = torch_to_jax(C)
        y_jax = jnp.einsum("bhpn, bn -> bhp", updated_ssm_state_jax, C)
        y = jax_to_torch(y_jax)
        print(f"  [Mamba2] y after einsum shape: {y.shape}")
        print(f"  [Mamba2] y after einsum sample values: {y.flatten()[:5]}")

        # JAXでのrearrange
        D = torch_to_jax(self.D)
        D_reshaped = jnp.reshape(D, (D.shape[0], 1))
        x_jax = torch_to_jax(x)
        y_jax = y_jax + D_reshaped * x_jax
        y = jax_to_torch(y_jax)
        print(f"  [Mamba2] y after adding D scaling (step) shape: {y.shape}")
        print(f"  [Mamba2] y after adding D scaling (step) sample values: {y.flatten()[:5]}")

        y_jax = jnp.reshape(y_jax, (y_jax.shape[0], -1))  # reshape equivalent to rearrange
        y = jax_to_torch(y_jax)
        print(f"  [Mamba2] y after rearrange to (b, d_inner) shape: {y.shape}")
        print(f"  [Mamba2] y after rearrange to (b, d_inner) sample values: {y.flatten()[:5]}")

        # RMSNormの処理は既存のコードをそのまま使用
        y_jax = torch_to_jax(y)
        y_jax = self.norm(y_jax, z)
        y = jax_to_torch(y_jax)
        print(f"  [Mamba2] y after RMSNorm (step) shape: {y.shape}")
        print(f"  [Mamba2] y after RMSNorm (step) sample values: {y.flatten()[:5]}")

        y_jax = jnp.dot(torch_to_jax(y), torch_to_jax(self.out_proj.weight).T) 
        y = jax_to_torch(y_jax)  # 結果をPyTorchに戻す
        print(f"  [Mamba2] y after out_proj (step) shape: {y.shape}")
        print(f"  [Mamba2] y after out_proj (step) sample values: {y.flatten()[:5]}")

        return y.unsqueeze(1), h


def segsum(x: Tensor, device: Device = None, use_jax: bool = True) -> Tensor:
    """Stable segment sum calculation.

    `exp(segsum(A))` produces a 1-semiseparable matrix, which is equivalent to a scalar SSM.

    Source: https://github.com/state-spaces/mamba/blob/219f03c840d5a44e7d42e4e728134834fddccf45/mamba_ssm/modules/ssd_minimal.py#L23-L32
    """
    T = x.size(-1)  # 時間次元のサイズ

    x_jax = torch_to_jax(x)
    # JAXでrepeat相当の処理
    x_jax = jnp.tile(x_jax[..., None], (1, 1, T))
    # 下三角行列のマスクをJAXで作成
    mask_jax = jnp.tril(jnp.ones((T, T), dtype=bool), k=-1)
    # JAXでmasked_fill相当の処理
    x_jax = jnp.where(mask_jax, x_jax, 0)
    # JAXで累積和 (cumsum)
    x_segsum_jax = jnp.cumsum(x_jax, axis=-2)
    # 再度マスクを適用 (下三角部分のみ残す)
    mask_jax = jnp.tril(jnp.ones((T, T), dtype=bool), k=0)
    x_segsum_jax = jnp.where(mask_jax, x_segsum_jax, -jnp.inf)

    x_segsum = jax_to_torch(x_segsum_jax)

    return x_segsum


from typing import Optional, Tuple
def ssd(
    x: Tensor,
    A: Tensor,
    B: Tensor,
    C: Tensor,
    chunk_size: int,
    initial_states: Optional[Tensor] = None,
    device: Optional[torch.device] = None
) -> Tuple[Tensor, Tensor]:
    """
    Structured State Space Duality (SSD) using JAX for internal computations.

    Arguments:
        x: (batch, seqlen, n_heads, d_head) - Input tensor
        A: (batch, seqlen, n_heads) - A tensor
        B: (batch, seqlen, n_heads, d_state) - B tensor
        C: (batch, seqlen, n_heads, d_state) - C tensor
        chunk_size: int - Size of each chunk
        initial_states: Optional[(batch, 1, n_heads, d_state, ...)] - Initial states
        device: Optional[torch.device] - Device to place the output tensors

    Returns:
        Y: (batch, seqlen, n_heads, d_head) - Output tensor
        final_state: (batch, n_heads, d_state, ...) - Final state tensor
    """
    print("[ssd_jax] Starting SSD computation with JAX...")
    assert x.shape[1] % chunk_size == 0, "Sequence length must be divisible by chunk_size."

    # Convert PyTorch tensors to JAX arrays
    x_jax = torch_to_jax(x)
    A_jax = torch_to_jax(A)
    B_jax = torch_to_jax(B)
    C_jax = torch_to_jax(C)

    # Rearrange into chunks using JAX's equivalent of einops.rearrange
    # Since JAX doesn't have einops, use jnp.reshape with appropriate axes
    batch, seqlen, n_heads, d_head = x.shape
    _, _, _, d_state = B.shape

    num_chunks = seqlen // chunk_size

    def rearrange_chunks(tensor, chunk_size):
        return tensor.reshape(batch, num_chunks, chunk_size, *tensor.shape[2:])

    x_jax = rearrange_chunks(x_jax, chunk_size)  # (batch, num_chunks, chunk_size, n_heads, d_head)
    A_jax = rearrange_chunks(A_jax, chunk_size)  # (batch, num_chunks, chunk_size, n_heads)
    B_jax = rearrange_chunks(B_jax, chunk_size)  # (batch, num_chunks, chunk_size, n_heads, d_state)
    C_jax = rearrange_chunks(C_jax, chunk_size)  # (batch, num_chunks, chunk_size, n_heads, d_state)

    print(f"  [ssd_jax] After chunking shapes -> x: {x_jax.shape}, A: {A_jax.shape}, B: {B_jax.shape}, C: {C_jax.shape}")
    print(f"  [ssd_jax] x after chunking sample values: {x_jax.flatten()[:5]}")
    print(f"  [ssd_jax] A after chunking sample values: {A_jax.flatten()[:5]}")
    print(f"  [ssd_jax] B after chunking sample values: {B_jax.flatten()[:5]}")
    print(f"  [ssd_jax] C after chunking sample values: {C_jax.flatten()[:5]}")

    # Rearrange A for cumulative sum
    A_jax = rearrange(A_jax, "b c l h -> b h c l")
    print(f"  [ssd_jax] A rearranged shape: {A_jax.shape}")
    print(f"  [ssd_jax] A rearranged sample values: {A_jax.flatten()[:5]}")

    A_cumsum_jax = jnp.cumsum(A_jax, axis=-1)
    print(f"  [ssd_jax] A_cumsum shape: {A_cumsum_jax.shape}")
    print(f"  [ssd_jax] A_cumsum sample values: {A_cumsum_jax.flatten()[:5]}")

    # 1. Compute the output for each intra-chunk (diagonal blocks)
    L_jax = jnp.exp(torch_to_jax(segsum(x=jax_to_torch(A_jax), device=device, use_jax=True)))
    print(f"  [ssd_jax] L shape: {L_jax.shape}")
    print(f"  [ssd_jax] L sample values: {L_jax.flatten()[:5]}")

    # Perform Einstein summation using JAX
    Y_diag_jax = jnp.einsum("bclhn, bcshn, bhcls, bcshp -> bclhp", C_jax, B_jax, L_jax, x_jax)
    print(f"  [ssd_jax] Y_diag shape: {Y_diag_jax.shape}")
    print(f"  [ssd_jax] Y_diag sample values: {Y_diag_jax.flatten()[:5]}")

    # 2. Compute the state for each intra-chunk
    decay_states_jax = jnp.exp(A_cumsum_jax[..., -1:] - A_cumsum_jax)
    print(f"  [ssd_jax] decay_states shape: {decay_states_jax.shape}")
    print(f"  [ssd_jax] decay_states sample values: {decay_states_jax.flatten()[:5]}")

    states_jax = jnp.einsum("bclhn, bhcl, bclhp -> bchpn", B_jax, decay_states_jax, x_jax)
    print(f"  [ssd_jax] states shape: {states_jax.shape}")
    print(f"  [ssd_jax] states sample values: {states_jax.flatten()[:5]}")

    # 3. Compute the inter-chunk SSM recurrence
    if initial_states is None:
        initial_states_jax = jnp.zeros_like(states_jax[:, :1])
        print("  [ssd_jax] Initialized initial_states with zeros.")
        print(f"  [ssd_jax] initial_states sample values: {initial_states_jax.flatten()[:5]}")
    else:
        initial_states_jax = torch_to_jax(initial_states)

    states_jax = jnp.concatenate([initial_states_jax, states_jax], axis=1)
    print(f"  [ssd_jax] states after concatenation shape: {states_jax.shape}")
    print(f"  [ssd_jax] states after concatenation sample values: {states_jax.flatten()[:5]}")

    # Compute decay_chunk
    A_cumsum_padded_jax = jnp.pad(A_cumsum_jax[:, :, :, -1], ((0,0), (0,0), (1,0)), mode='constant')
    decay_chunk_jax = jnp.exp(torch_to_jax(segsum(x=jax_to_torch(A_cumsum_padded_jax), device=device, use_jax=True)))
    print(f"  [ssd_jax] decay_chunk shape: {decay_chunk_jax.shape}")
    print(f"  [ssd_jax] decay_chunk sample values: {decay_chunk_jax.flatten()[:5]}")

    new_states_jax = jnp.einsum("bhzc, bchpn -> bzhpn", decay_chunk_jax, states_jax)
    print(f"  [ssd_jax] new_states shape: {new_states_jax.shape}")
    print(f"  [ssd_jax] new_states sample values: {new_states_jax.flatten()[:5]}")

    states_jax, final_state_jax = new_states_jax[:, :-1], new_states_jax[:, -1]
    print(f"  [ssd_jax] states shape after splitting: {states_jax.shape}, final_state shape: {final_state_jax.shape}")
    print(f"  [ssd_jax] states after splitting sample values: {states_jax.flatten()[:5]}")
    print(f"  [ssd_jax] final_state sample values: {final_state_jax.flatten()[:5]}")

    # 4. Compute state -> output conversion per chunk
    state_decay_out_jax = jnp.exp(A_cumsum_jax)
    print(f"  [ssd_jax] state_decay_out shape: {state_decay_out_jax.shape}")
    print(f"  [ssd_jax] state_decay_out sample values: {state_decay_out_jax.flatten()[:5]}")

    Y_off_jax = jnp.einsum("bclhn, bchpn, bhcl -> bclhp", C_jax, states_jax, state_decay_out_jax)
    print(f"  [ssd_jax] Y_off shape: {Y_off_jax.shape}")
    print(f"  [ssd_jax] Y_off sample values: {Y_off_jax.flatten()[:5]}")

    # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)
    Y_jax = Y_diag_jax + Y_off_jax
    Y_jax = rearrange(Y_jax, "b c l h p -> b (c l) h p")
    print(f"  [ssd_jax] Y after adding Y_diag and Y_off shape: {Y_jax.shape}")
    print(f"  [ssd_jax] Y after adding Y_diag and Y_off sample values: {Y_jax.flatten()[:5]}")

    # Convert JAX arrays back to PyTorch tensors
    Y = jax_to_torch(Y_jax)
    final_state = jax_to_torch(final_state_jax)

    return Y, final_state


class RMSNorm(torch.nn.Module):
    #def __init__(self, d: int, eps: float = 1e-5, device=None, use_jax=False):
    def __init__(self, d: int, eps: float = 1e-5, device=None, use_jax=True):
        """Gated Root Mean Square Layer Normalization

        Paper: https://arxiv.org/abs/1910.07467
        """
        super().__init__()
        self.eps = eps
        self.weight = torch.nn.Parameter(torch.ones(d, device=device))
        self.use_jax = use_jax  # JAXを使うかどうかを選択
        print(f"[RMSNorm] Initialized RMSNorm with d={d}, eps={eps}")
        print(f"[RMSNorm] weight shape: {self.weight.shape}")
        print(f"[RMSNorm] weight sample values: {self.weight.flatten()[:5]}")

    def forward(self, x, z=None):
        if z is not None:
            # SiLUによるゲーティングを適用
            x = x * silu(z, self.use_jax)
            print(f"[RMSNorm] x after gated scaling shape: {x.shape}")
            print(f"[RMSNorm] x after gated scaling sample values: {x.flatten()[:5]}")

        if self.use_jax:
            # JAXでのRMSNorm計算
            print("[RMSNorm] Using JAX for the RMSNorm computation")
            x_jax = torch_to_jax(x)
            mean_sq_jax = jnp.mean(jnp.square(x_jax), axis=-1, keepdims=True)
            rsqrt_jax = jnp.sqrt(1 / (mean_sq_jax + self.eps))
            x_normalized_jax = x_jax * rsqrt_jax * torch_to_jax(self.weight)
            x_normalized = jax_to_torch(x_normalized_jax)

        else:
            # PyTorchでのRMSNorm計算
            print("[RMSNorm] Using PyTorch for the RMSNorm computation")
            mean_sq = x.pow(2).mean(-1, keepdim=True)
            print(f"[RMSNorm] Mean squared value shape: {mean_sq.shape}")
            print(f"[RMSNorm] mean_sq sample values: {mean_sq.flatten()[:5]}")
            rsqrt = torch.rsqrt(mean_sq + self.eps)
            print(f"[RMSNorm] rsqrt shape: {rsqrt.shape}")
            print(f"[RMSNorm] rsqrt sample values: {rsqrt.flatten()[:5]}")
            x_normalized = x * rsqrt * self.weight

        print(f"[RMSNorm] x_normalized shape: {x_normalized.shape}")
        print(f"[RMSNorm] x_normalized sample values: {x_normalized.flatten()[:5]}")
        return x_normalized


class RMSNorm(torch.nn.Module):
    def __init__(self, d: int, eps: float = 1e-5, device=None, use_jax=True):
        """Gated Root Mean Square Layer Normalization

        Paper: https://arxiv.org/abs/1910.07467
        """
        super().__init__()
        self.eps = eps
        self.weight = torch.nn.Parameter(torch.ones(d, device=device))
        self.use_jax = use_jax  # JAXを使うかどうかを選択
        print(f"[RMSNorm] Initialized RMSNorm with d={d}, eps={eps}")
        print(f"[RMSNorm] weight shape: {self.weight.shape}")
        print(f"[RMSNorm] weight sample values: {self.weight.flatten()[:5]}")

    def forward(self, x, z=None):
        if z is not None:
            x = x * silu(z, self.use_jax)
            print(f"[RMSNorm] x after gated scaling shape: {x.shape}")
            print(f"[RMSNorm] x after gated scaling sample values: {x.flatten()[:5]}")

        mean_sq = jnp.mean(jnp.square(x), axis=-1, keepdims=True)
        rsqrt = jnp.sqrt(1 / (mean_sq + self.eps))
        x_normalized = x * rsqrt * torch_to_jax(self.weight)

        print(f"[RMSNorm] x_normalized shape: {x_normalized.shape}")
        print(f"[RMSNorm] x_normalized sample values: {x_normalized.flatten()[:5]}")
        return x_normalized

def silu(x, use_jax=True):
    """Applies the Sigmoid Linear Unit (SiLU), element-wise.

    This function supports both JAX and PyTorch computations depending on the use_jax flag.
    """
    return x * jax.nn.sigmoid(x)
